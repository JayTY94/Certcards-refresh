





1
Power BI Desktop
Storage mode: With storage mode, you can now specify which visuals require a query to back-end data sources. Visuals that don't require a query are imported even if they're based on DirectQuery. This feature helps improve performance and reduce back-end load. Previously, even simple visuals, such as slicers, initiated queries that were sent to back-end sources.





2
Power BI Desktop
The Dual storage mode is a performance optimization. It should be used only in ways that don't compromise the ability to meet business requirements. For alternative behavior, consider using the techniques described in the Many-to-many relationships in Power BI Desktop.





3
Power Platform administration
You use the Microsoft 365 admin center to create user accounts for every user who needs access to apps, flows, or chatbots. The user account registers the user with Microsoft Online Services environment.





4
Power Platform administration
When you create a user and assign a license in the Microsoft 365 admin center, the user is also created in customer engagement apps. It can take a few minutes to complete the synchronization process between the Microsoft 365 admin center and customer engagement apps.





5
Site Scraping
I had to use the os.path abspath function to get the absolute path of the file I created for the sake of sending it as an email attachment.
  from os.path import abspath
  ...
  newmail.Attachments.Add(Source = abspath(relative_output_path))
  newmail.Send()





6
Site Scraping
Use Selenium to navigate to desirable data - this involved moving the driver focus to iframes and then interacting with elements there
  iframe = driver.find_elements(By.ID, 'tool')[0]
  driver.switch_to.frame(iframe)
  ...
  driver.switch_to.parent_frame()
  driver.switch_to.frame('data')
  sleep(3)





7
MS Data Analyst
At report design time in Microsoft Power BI Desktop, you can create measures (except when the model is a live connection to SQL Server Analysis Services multidimensional model). These measures belong to the report, and so they're called report-level measures.





8
MS Data Analyst
To create a mobile-optimized version of your report, you can:

Design a mobile layout view, where you can drag and drop certain visuals onto a phone emulator canvas.
Use visuals and slicers that are suitable for use on small, mobile screens.





9
MS Data Analyst
To learn how to run a network port test, see Adjust communication settings for the on-premises data gateway.
To get information on how to provide proxy information for your gateway, see Configure proxy settings for the on-premises data gateway.
To find the current data center region that you're in, see Set the data center region.





10
Maths
Compositional data refers to vectors of positive measurements which carry relative information, commonly analyzed in parts-per-whole relationships.





11
Maths
Regularity conditions are mathematical conditions or assumptions required for certain statistical methods to be valid, like smoothness and boundedness of functions involved in estimation or hypothesis testing.









13
OAuth2.0
Access tokens are short lived. Refresh them after they expire to continue accessing resources. You can do so by submitting another POST request to the /token endpoint. Provide the refresh_token instead of the code. Refresh tokens are valid for all permissions that your client has already received consent for.





14
OAuth2.0
Use the auth code flow paired with Proof Key for Code Exchange (PKCE) and OpenID Connect (OIDC) to get access tokens and ID tokens in these types of apps:

Single-page web application (SPA)
Standard (server-based) web application
Desktop and mobile apps





15
OAuth2.0
To learn who the user is before redeeming an authorization code, it's common for applications to also request an ID token when they request the authorization code. This approach is called the hybrid flow because it mixes OIDC with the OAuth2 authorization code flow.





16
Power Platform
UI Component: View
Views define how a list of rows for a specific table is displayed in your application. A view defines the columns to display, width of each column, sort behavior, and the default filters.





17
Power Platform
UI Component: Custom page (preview)
	A canvas based page which allows flexible layout, low-code Fx functions, and Power Apps connector data. For more information, see Model-driven app custom page overview (preview)





18
Power Platform
Types of Logic Components:
  Business process flow
  Workflow
  Actions
  Business rule
  Power Automate Flow





19
RAG
One prevalent challenge when implementing language models through chat is the so-called groundedness, which refers to whether a response is rooted, connected, or anchored in reality or a specific context. In other words, groundedness refers to whether the response of a language model is based on factual information.





20
RAG
You can use Azure AI Studio to build a custom copilot that uses your own data to ground prompts. Azure AI Studio supports a range of data connections that you can use to add data to a project, including:

Azure Blob Storage
Azure Data Lake Storage Gen2
Microsoft OneLake
You can also upload files or folders to the storage used by your AI Studio project.





21
RAG
Look up relevant information
This is a node in the sample RAG Prompt flow. you use the Index Lookup tool to query the search index you created with the integrated Azure AI Search feature and find the relevant information from your data source.





22
Azure Functions Quickstart
The issue was an attempt to install the deprecated azure meta-package, which is no longer supported. The resolution involved installing the azure-functions package specifically for Python Azure Functions, enabling the successful setup and execution of the Azure Functions Python application.





23
Azure Functions Quickstart
We recommend setting up continuous deployment so that your function app in Azure is updated when you update source files in the connected source location. You can also deploy your project files from Visual Studio Code. When you publish from Visual Studio Code, you can take advantage of the Zip deploy technology.





24
Azure Functions Quickstart
Maintenance: Meta-packages need to be maintained just like any other package. They should be updated to point to the latest, stable releases of their dependencies. If not properly maintained, they might pin outdated versions or lead to dependency conflicts.





25
Azure Functions Quickstart
Definition: A meta-package is a type of Python package that doesn't contain actual modules or libraries itself but is a collection of dependencies on other packages. It simplifies installation by grouping related packages under a single installable entity.





26
Developer Mode
The Windows SDK provides access to all of the APIs and development features exposed by the Windows OS. The Windows SDK is required for building Windows apps as well as other types of components (such as services and drivers). The latest Windows SDK is installed with Visual Studio 2019 and Visual Studio 2022 by default.





27
Developer Mode
The Windows App SDK was previously known by the code name Project Reunion. Some SDK assets (such as the VSIX extension and certain NuGet packages) still use this name, but these assets will be renamed in a future release. Some documentation still uses Project Reunion when referring to an existing asset or a specified earlier release.





28
Developer Mode
Developer Mode replaces the Windows 8.1 requirements for a developer license. In addition to sideloading, the Developer Mode setting enables debugging and additional deployment options. This includes starting an SSH service to allow deployment to this device. In order to stop this service, you need to disable Developer Mode.





29
Developer Mode
When you enable Developer Mode on desktop, a package of features is installed, including:

Windows Device Portal: Device Portal is only enabled (and firewall rules are only configured for it) when the Enable Device Portal option is turned on.
Installs and configures firewall rules for SSH services that allow remote installation of apps. Enabling Device Discovery will turn on the SSH server.





30
Kaggle
In Lesson 1, we engineered our time dummy in Pandas directly. From now on, however, we'll use a function from the statsmodels library called DeterministicProcess. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on.





31
Kaggle
Once we've identified the shape of the trend, we can attempt to model it using a time-step feature. We've already seen how using the time dummy itself will model a linear trend:
target = a * time + b

We can fit many other kinds of trend through transformations of the time dummy. If the trend appears to be quadratic (a parabola), we just need to add the square of the time dummy to the feature set, giving us:
target = a * time ** 2 + b * time + c





32
Kaggle
I created a shifted, lag feature for my time series using the following syntax. Turns out the pandas shift() function will respect the groupby() groups, in this case the different engagements.

df['cumulative_hours_lag'] = df.groupby('EngagementID')['cumulative_hours'].shift(1)





33
Kaggle
Moving Average Plots
To see what kind of trend a time series might have, we can use a moving average plot. To compute a moving average of a time series, we compute the average of the values within a sliding window of some defined width. The idea is to smooth out any short-term fluctuations in the series so that only long-term changes remain.





34
numpy
numpy.where(condition, [x, y, ]/)

First of all, the definition ends with a forward slash (/) character. You might think this represents a division or line continuation symbol, but it’s neither. By placing the forward slash special parameter at the end, the documentation is telling you that each parameter passed must be passed by position and not by keyword.





35
numpy
For example, you could have created your own randomized version of test_array using the following:

>>> test_array = np.random.uniform(low=-10, high=10, size=(3, 4))
>>> test_array
array([[-2.71697178, -2.49701546, -7.57662054, -9.41817892],
       [ 2.43095102,  0.7143025 ,  0.25938839, -4.78215376],
       [-7.13802191, -5.47446998, -0.47173589, -0.36727671]])





36
numpy
Two arrays are broadcast compatible if their rightmost dimensions are identical, or either of these dimensions is 1. Once your arrays are broadcast compatible, you can use them together with the where() function.





37
numpy
You would typically use np.where() when you have an array and need to analyze its elements differently depending on their values. For example, you might need to replace negative numbers with zeros or replace missing values such as None or np.nan with something more meaningful. When you run where(), you’ll produce a new array containing the results of your analysis.





38
Azure OpenAI
Assistants API supports persistent automatically managed threads. This means that as a developer you no longer need to develop conversation state management systems and work around a model’s context window constraints. The Assistants API will automatically handle the optimizations to keep the thread below the max context window of your chosen model.





39
Azure OpenAI
Assistants can access files in several formats. Either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (such as images or spreadsheets) and cite files they reference in the Messages they create.





40
Azure OpenAI
Assistant 
Custom AI that uses Azure OpenAI models in conjunction with tools.

Message
A message created by an Assistant or user. Messages can include text, images, and other files. Messages are stored as a list on the Thread.





41
Azure OpenAI
Run
Activation of an Assistant to begin running based on the contents of the Thread.

Run Step
A detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create messages during its run. Examining Run Steps allows you to understand how the Assistant is getting to its final results.





42
Azure OpenAI
Thread
A conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model's context.





43
Azure Storage
Online analytical processing (OLAP) systems commonly support fewer users, have longer response times, can be less available, and typically handle large transactions or complex transactions.
An example of an Azure service that supports OLAP is Azure Analysis Services.





44
Azure Storage
Transactional databases are often called online transaction processing (OLTP) systems. OLTP systems commonly support many users, have quick response times, and handle large volumes of data. They also are highly available, which means they have minimal downtime. OLTP systems typically handle small transactions or relatively simple transactions.
An example of an Azure service that supports OLTP is Azure SQL Database.





45
Azure Storage
Each storage account requires some time and attention from an administrator to create and maintain. It also increases complexity for anyone who adds data to your cloud storage. Everyone in an administrator role needs to understand the purpose of each storage account so they add new data to the correct account.





46
Azure Storage
If a change to one piece of data must result in a change to another piece of data, an application needs to group together a series of data updates. You can use transactions to group these updates. In a transaction, if one event in a series of updates fails, the entire series can be rolled back or undone.





47
Azure Storage
A storage account represents a collection of settings like location, replication strategy, and subscription owner. You need one storage account for each group of settings that you want to apply to your data. The following illustration shows two storage accounts that differ in one setting (region); that one difference is enough to require separate storage accounts.





48
OData Requests
Deserialization in APIs:
The process of converting data from a JSON or serialized format into a usable object in the API server. Errors can arise if the input data does not match the expected schema or types during deserialization.





49
OData Requests
Logical Name (Entity):
The system name used to refer to an entity in Dynamics 365. It is different from the display name and is used when interacting with the API, such as in @odata.bind syntax for lookups.





50
OData Requests
PATCH Request:
A type of HTTP request used to update partial resources in an API. Unlike PUT, which replaces an entire resource, PATCH only modifies specified fields, allowing for more efficient updates.





51
OData Requests
Form Data in HTTP Requests:
A method of sending key-value pairs through HTTP requests, typically used when submitting form-like data. In JSON-based APIs, form data is less common, with JSON payloads being the standard.





52
OData Requests
JSON null:
The JSON equivalent of Python's None, used to represent missing or undefined data. It’s essential to replace invalid values like NaN with null in JSON payloads to avoid request errors.





53
UHero Requests
Custom Components (Angular):
Custom components are reusable blocks of code in frameworks like Angular that define new HTML elements. They encapsulate logic, styling, and behavior, making it difficult to scrape using traditional methods like targeting <table> or <div> tags.





54
UHero Requests
execute_script (Selenium):
execute_script is a Selenium method that allows you to execute raw JavaScript code within the browser being controlled. It’s often used to retrieve hidden variables or perform actions not directly available through Selenium commands.





55
UHero Requests
lib-geo-selector / Custom HTML Tags:
In Angular and similar frameworks, custom HTML tags like <lib-geo-selector> are used to represent components with built-in functionality. These tags can be harder to scrape since they aren't standard HTML elements like <select> or <table>.





56
UHero Requests
Network Tab (in DevTools):
The Network tab in browser DevTools tracks all network activity, including API calls (XHR, Fetch requests), CSS, and JavaScript loading. It's especially useful for identifying where data comes from and if an API can be used for scraping.





57
UHero Requests
XHR (XMLHttpRequest):
XHR is a JavaScript object that allows websites to make requests to servers asynchronously, loading data (e.g., JSON or HTML fragments) without requiring a page reload. Many modern web apps use this to dynamically update content.





58
Azure VDI Project
The terraform plan -refresh-only option generates a plan that only refreshes the state without proposing any changes to infrastructure. This is useful for seeing the current state of resources, particularly when changes have been made outside of Terraform, allowing you to compare infrastructure with the configuration.





59
Azure VDI Project
The terraform state command family allows you to manipulate the state file directly. Commands like terraform state show, terraform state mv, and terraform state rm enable you to view, move, or remove resources from the state. This is useful for fixing discrepancies or reorganizing resources without affecting the actual infrastructure.





60
Azure VDI Project
The flow_timeout_in_minutes parameter in Azure Virtual Network (VNet) controls how long a network flow (TCP/UDP session) remains active when no data is being transferred. The value can range from 4 to 30 minutes, with 4 being the minimum, to prevent idle connections from being prematurely terminated.





61
Azure VDI Project
1. You have a group of developers that need virtual desktops that they can manage. What type of host pool do you create for them? 

With a personal host pool, each user has their own dedicated VM. They can be a local administrator for their VM to manage app installation.





62
Azure VDI Project
Unconfigurable Attribute Error
This Terraform error occurs when you try to manually set an attribute that Terraform automatically manages, such as an id for subnets. Terraform expects such attributes to be generated dynamically during resource creation, and manually setting them can result in conflicts between the configuration and the actual state.





63
Azure VDI Project
The terraform plan -target= command allows you to run a plan for specific resources, focusing Terraform's execution on just those resources instead of the entire configuration. It’s useful for debugging and making changes to certain resources without affecting others, especially in large environments.





64
Dataverse
Confidential Client:

Definition: An application type in OAuth2 that requires a client secret or assertion, typically used by server-based or service applications.
Context: Initially, your app was likely configured as a confidential client, which caused the error when we tried using Device Code Flow.





65
Dataverse
Authorization Code Flow:

Definition: An OAuth2 flow used by public clients (like single-page apps or mobile apps) where the user signs in interactively, providing credentials, and possibly undergoing multi-factor authentication (MFA).
Context: We discussed this flow as an alternative to the Client Credentials Flow because you wanted MFA to be required.





66
Dataverse
Event-Driven Architecture: Plug-ins operate on an event-driven architecture, meaning they are triggered by specific events within the Dataverse environment. These events can be actions like data creation, update, or deletion. Understanding the event model is crucial for effectively implementing and managing plug-ins.





67
Dataverse
Bearer Token:

Definition: A type of token used in OAuth2 that authorizes access to resources like APIs.
Context: The token obtained from Azure AD that you include in the Authorization header for your API requests.





68
Dataverse
Public Client:

Definition: An application type in OAuth2 that does not require a client secret because it runs on devices that cannot securely store credentials (like mobile or desktop apps).
Context: We configured your app as a public client so it could authenticate without a client secret





69
Dataverse
MSAL (Microsoft Authentication Library):

Definition: A Microsoft library for handling authentication and obtaining tokens using OAuth2. It simplifies the process of integrating OAuth2 with applications.
Context: We used MSAL in Python to implement the Device Code Flow and authenticate interactively with MFA.





70
Diffusers Documentation
Sometimes, old environment paths can linger in the registry or elsewhere in the system’s path cache, causing confusion about which Python executable should be used. Running python -m pip explicitly tells the Python interpreter itself to find the correct pip module, sidestepping any possible confusion from old, broken pip links.





71
Diffusers Documentation
On Windows, pip relies on a small launcher executable that links to the correct Python interpreter and pip package. If Python was uninstalled and reinstalled in a new location, the link in that launcher may be broken or still pointing to the old installation path, even if the correct paths are in the Path variable. This causes pip alone to fail, as it tries to find the old, non-existent Python.





72
Diffusers Documentation
The Stable Diffusion model is a good starting point, and since its official launch, several improved versions have also been released. However, using a newer version doesn’t automatically mean you’ll get better results. You’ll still have to experiment with different checkpoints yourself, and do a little research (such as using negative prompts) to get the best results.





73
Diffusers Documentation
Schedulers manage going from a noisy sample to a less noisy sample given the model output - in this case, it is the noisy_residual.





74
Diffusers Documentation
The Scripts\ folder contains the pip executable. If there’s any issue in the way Path variables are processed (which can occasionally happen if multiple Python versions are installed), running python -m pip ensures the right pip is used with the right Python version.





75
Diffusers Documentation
Choosing a more efficient scheduler could help decrease the number of steps without sacrificing output quality. You can find which schedulers are compatible with a different model in the DiffusionPipeline by calling the compatibles method:

pipeline.scheduler.compatibles





76
Git
git status: This command shows the current state of your working directory and staging area. It helps you see which changes have been staged, which haven't, and which files aren't being tracked by Git.





77
Git
git pull: This command fetches and integrates changes from a remote repository into your local repository. It's a combination of git fetch and git merge.





78
Git
git init: This command initializes a new Git repository in your current directory. It creates a hidden .git directory that tracks all changes to your project.





79
Git
git diff: This command shows the differences between commits, branches, or your working directory and the staging area. It's useful for reviewing changes before committing them.





80
Git
git log: This command shows the commit history for the repository. It helps you see what changes were made, when, and by whom.





81
Git
git stash: This command temporarily saves changes in your working directory that you aren't ready to commit. You can later apply or discard these changes.





82
OData
$expand
The $expand query option is used to include related entities in the response. For example, if you are querying a customer entity, you can use $expand to retrieve their associated orders or addresses in the same query, enabling easier navigation of related data.





83
OData
$skip
The $skip query option is used to skip a specified number of results from the start of the result set. It is commonly used in combination with $top to paginate through large data sets, allowing clients to retrieve data in chunks.





84
OData
Metadata Document
In OData, the metadata document describes the structure of the OData service, including the entity types, relationships, and properties. It is expressed in an XML format and allows developers and tools to understand the underlying schema of the OData service for integration purposes.





85
OData
Action
Actions in OData are operations that can perform modifications on the data, such as updating entities, triggering workflows, or processing business logic. Unlike functions, actions can alter the data or system state and are invoked through POST requests.





86
OData
Service Document
The service document is the entry point for an OData service. It provides metadata about the available entity sets and other resources that the client can interact with. The document acts as a map, allowing the client to discover what data is exposed via the OData service.





87
OData
Key
A key is a unique identifier for an entity within an entity set. In OData, keys are used to locate and identify individual entities, much like primary keys in relational databases. They are essential for accessing specific resources via URLs.





88
Workera.ai
Azure Container Network Interface (CNI) plug-ins in Azure are designed to seamlessly integrate with Azure networking services. This integration ensures that containers can communicate with other resources in the Azure environment and take advantage of features like virtual networks, load balancers, and security groups.





89
Workera.ai
Azure Resource Groups provide a logical way to organize and manage resources within the Azure cloud. A resource group is a container that holds related resources for an Azure solution, such as virtual machines, storage accounts, and virtual networks.





90
Workera.ai
Automatic Replication and Failover: Certain services, such as Azure Storage, automatically replicate data between region pairs. This replication helps ensure that your data is always available, even if one region experiences an outage. Additionally, Azure prioritizes the restoration of services in region pairs to ensure a quicker recovery time.





91
Workera.ai
Azure HDInsight: Suitable for diverse big data processing needs with open-source frameworks but requires infrastructure management.





92
Workera.ai
Azure IoT Hub
Purpose and Use Cases:

Core IoT Service: Azure IoT Hub is a foundational service designed to provide the connectivity and messaging infrastructure required for building sophisticated IoT solutions.
Customization: It's ideal for developers who need a highly customizable solution and have the expertise to build and manage the IoT application layer themselves.





93
Workera.ai
Azure DevTest Labs enables quick provisioning of resources such as virtual machines, allowing development and testing teams to create environments on-demand. This is particularly useful for scenarios where teams need to replicate production environments for testing purposes.





94
Conditional Access
Report-only: Not applied	
Not all configured policy conditions were satisfied. For example, the user is excluded from the policy or the policy only applies to certain trusted named locations.





95
Conditional Access
Find more information about the problem by clicking More Details in the initial error page. Clicking More Details will reveal troubleshooting information that's helpful when searching the Microsoft Entra sign-in events for the specific failure event the user saw or when opening a support incident with Microsoft.
Stuff like App name, App id, IP address, Device identifier, Device Platform,





96
Conditional Access
During sign-in, policies in report-only mode are evaluated but not enforced.
Results are logged in the Conditional Access and Report-only tabs of the Sign-in log details.
Customers with an Azure Monitor subscription can monitor the impact of their Conditional Access policies using the Conditional Access insights workbook.





97
Conditional Access
Conditional Access App Control enables user app access and sessions to be monitored and controlled in real time based on access and session policies. Access and session policies are used within Microsoft Defender for Cloud Apps portal to further refine filters and set actions to be taken on a user.





98
Conditional Access
Conditional Access: Client certificates can be used in conditional access policies to control access based on device compliance. This adds an extra layer of security by ensuring that only compliant devices can access sensitive data.





99
Conditional Access
Token Renewal: Refresh tokens are used to obtain new access and refresh token pairs when the current access token expires.
Token Lifetime: The default lifetime for refresh tokens is 24 hours for single-page applications and 90 days for other scenarios.





100
Conditional Access
Report-only: Success	
All configured policy conditions, required non-interactive grant controls, and session controls were satisfied. For example, a multifactor authentication requirement is satisfied by an MFA claim already present in the token, or a compliant device policy is satisfied by performing a device check on a compliant device.





101
Dataverse Plugins
The Plugin Execution Pipeline defines the sequence of events where plugins can be triggered in Dataverse. The primary stages include:

Pre-validation: Executes before the main system operation, allowing for early validation.
Pre-operation: Runs after the main operation is validated but before it's committed to the database.
Post-operation: Occurs after the main operation has been completed and committed. Understanding these stages is crucial for placing plugins at the appropriate point in the data processing workflow, ensuring optimal performance and data integrity.





102
Dataverse Plugins
Plugins are custom business logic components that execute in response to specific events within Dataverse, such as record creation or updates. Written in .NET, plugins can enforce business rules, validate data, or integrate with external systems. They are registered against specific messages (like Create, Update) and entities, allowing for granular control over when and how custom logic is triggered within the Dataverse environment.





103
Dataverse Plugins
Filtering Attributes are specific fields defined during the registration of a plugin that determine when the plugin should execute. For instance, a plugin might be set to trigger only when certain attributes of an entity are updated. By specifying these attributes, Dataverse optimizes performance by invoking the plugin only when relevant data changes occur, rather than on every update to the entity.





104
Dataverse Plugins
SDK Message Processing Steps represent the registration details of a plugin within Dataverse. Each step defines when (which message and stage) and how a plugin executes. Key properties include the message (e.g., Update), primary entity, execution pipeline stage (Pre-validation, Pre-operation, Post-operation), and filtering attributes. These steps orchestrate the plugin's behavior in response to data operations.





105
Dataverse Plugins
API Permissions and Scopes define what operations an application can perform and what data it can access within an API like Dataverse Web API. During Azure AD application registration, permissions are granted (e.g., user_impersonation) to control access levels. Scopes specify the exact permissions requested during the authentication process. Proper configuration ensures that applications have the necessary access without over-privileging, adhering to the principle of least privilege for security.





106
Dataverse Plugins
Error Handling in Dataverse involves interpreting and managing error messages returned by the system during operations. Errors can arise from validation failures, plugin restrictions, permission issues, or data conflicts. Effective error handling requires understanding error codes, messages, and contextual details provided by the system. This enables developers to debug issues, enforce business rules, and provide meaningful feedback to users or calling applications.





107
Dataverse Plugins
The PluginType entity in Dataverse stores metadata about registered plugins. It includes information such as the plugin's unique identifier (plugintypeid), name, assembly details, and the class implementing the plugin logic. By querying the plugintypes entity via the Web API, developers can retrieve information about specific plugins, facilitating management and debugging of custom business logic.





108
Dataverse Queries
Pagination and $top
Definition: Pagination refers to breaking down large sets of data into smaller, more manageable chunks. The $top query parameter controls how many records are returned in a single request.
Relevance: For performance reasons, Dataverse limits the number of records returned in a single API request. Pagination, along with $top, helps in retrieving large datasets incrementally without overwhelming the system.





109
Dataverse Queries
$expand
Definition: An OData query option that allows retrieving related data in a single query by expanding related entities.
Relevance: The $expand option is highly useful when you need to retrieve related data (e.g., including a parent entity's information in the same query). It reduces the number of separate API calls required to retrieve related data, optimizing performance.





110
Dataverse Queries
FetchXML
Definition: A proprietary query language used in Dataverse for more advanced querying capabilities. Although the Web API primarily supports OData queries, FetchXML can be used for complex querying scenarios such as joins, aggregates, and advanced filtering.
Relevance: FetchXML is particularly useful for scenarios where OData query limitations are encountered. It can be executed using the Web API through the RetrieveMultiple message.





111
Dataverse Queries
Entity Bound Actions
Definition: These are custom or system-defined actions that are tied to a specific entity and can be invoked via the Web API to perform tasks or trigger workflows.
Relevance: Entity-bound actions are useful for performing more complex business logic or automation that is triggered based on a record in an entity. For example, triggering a workflow on an engagement task when it's completed.





112
Dataverse Queries
_ts_engagementid_value@Microsoft.Dynamics.CRM.lookuplogicalname
Definition: A field that contains the logical name of the entity that the lookup field points to. This helps identify the type of entity related to the GUID in the _ts_engagementid_value field.
Example: The value "ts_engagement" indicates that the lookup points to the ts_engagement entity.





113
Dataverse Queries
Change Tracking
Definition: A feature that allows the tracking of changes made to data in Dataverse entities. The Web API exposes a mechanism to track when records have been created, updated, or deleted.
Relevance: Useful for scenarios like synchronizing data between systems or auditing changes in your data. The $deltatoken query parameter is used to fetch only the changes since the last request, improving efficiency.





114
Dataverse Queries
Optimistic Concurrency
Definition: A method of ensuring that data integrity is maintained when multiple users or processes are updating the same record. It checks if the record has been modified since it was last retrieved before allowing updates.
Relevance: This is useful for preventing conflicts in multi-user environments. The Web API uses the If-Match header to implement this mechanism, ensuring that updates happen only if the record has not been altered by another process.





115
AKS 
Web App for Containers is a feature of Azure App Service, a fully managed service for hosting HTTP-based web apps with built-in infrastructure maintenance, security patching, scaling, and diagnostic tooling. For more information, see App Service documentation.





116
AKS 
AKS is commonly used to host multiple workloads or disparate workload components. You can isolate these workloads and components by using Kubernetes native functionality, like namespaces, access controls, and network controls, to meet security requirements.





117
AKS 
Azure Kubernetes Service (AKS) is a managed Kubernetes service for running containerized applications. With AKS, you can take advantage of managed add-ons and extensions for additional capabilities while preserving the broadest level of configurability. For more information, see AKS documentation.





118
AKS 
Although AKS offers the most customization, it demands greater operational input. In contrast, PaaS solutions like Container Apps and Web App for Containers let Azure handle tasks like OS updates.





119
AKS 
Application scalability in AKS is the sole responsibility of the customer. Container Apps and Web App for Containers offer more streamlined approaches.





120
AKS 
Azure Container Apps is a fully managed Kubernetes-based application platform that helps you deploy HTTP and non-HTTP apps from code or containers without orchestrating infrastructure. For more information, see Azure Container Apps documentation.





121
AKS 
Web App for Containers and Container Apps health probe configurations are more streamlined than those of AKS, given that they use the familiar Azure Resource Manager API. AKS requires the use of the Kubernetes API.





122
AKS 
AKS also provides access to the Kubernetes API server, which enables you to customize the container orchestration and thus deploy projects from the Cloud Native Computing Foundation (CNCF). Consequently, there's a significant learning curve for workload teams that are new to Kubernetes.





123
General
Hotpatching: A feature that allows for installing updates on virtual machines without the need for a reboot, minimizing downtime and disruption.





124
General
The locale.setlocale() function is used to set the locale explicitly in your code, while the locale.currency() function formats currency values according to the specified locale.





125
General
Sideloading involves installing apps on a device using methods other than official app stores, often for accessing region-restricted apps or testing development versions.





126
General
A normal distribution is mesokurtic, serving as a baseline with moderate tails and a kurtosis value of 0 (excess kurtosis) or 3 (raw kurtosis).





127
General
[Win] + [ctrl] + [v] opens the windows audio device selection menu in Windows 11. Very helpful.





128
General
Sectoral Diversity: Within the European Stoxx 600, there is a rich mix of sectors represented, including financial services, healthcare, industrials, technology, and consumer goods. This diversity not only mitigates investment risk but also offers investors comprehensive exposure to the varied economic sectors across Europe.





129
General
Automatic VM guest patching: This refers to the automated process of updating the operating system of virtual machines hosted on Azure to ensure security and compliance with the latest patches.





130
General
Sideloading benefits include testing new app features, deploying custom enterprise apps, and accessing legacy applications critical to business operations, despite not being available in app stores.





131
pandas
Control Over Record Path and Metadata in json_normalize: With json_normalize, you can specify the path to the records you want to normalize using the record_path parameter. The meta parameter in json_normalize allows you to include metadata from parent levels, ensuring hierarchical data is accurately represented.





132
pandas
Expanding List-Like Elements into Rows using explode: The explode method transforms each element of a list-like column into separate rows by applying DataFrame.explode. This allows you to analyze each list element individually in a row-wise fashion.





133
pandas
Custom Separators and Maximum Depth in json_normalize: The sep parameter in json_normalize lets you define a custom separator for nested keys, defaulting to a dot (.). The max_level parameter allows you to limit how deeply json_normalize flattens nested data, providing control over the normalization process.





134
pandas
Error Handling and Performance with json_normalize: Using the errors parameter in json_normalize, you can manage parsing errors with options like 'raise' or 'ignore'. Efficient handling of large JSON datasets with json_normalize improves performance, making it valuable for big data applications.





135
pandas
Column-Specific Operation with explode: You can apply explode to a specific column by passing the column name to DataFrame.explode. Other columns remain unaffected, with their values duplicated alongside the new rows, maintaining the DataFrame's structural integrity.





136
pandas
Handling Empty Lists and Missing Data in explode: The explode method gracefully handles empty lists and missing values (NaN) when you use DataFrame.explode. Empty lists result in no additional rows, while NaN values are propagated, ensuring DataFrame consistency after the operation.





137
pandas
Exploding Multiple Columns using explode: Starting from pandas version 1.3.0, DataFrame.explode allows you to explode multiple columns simultaneously by passing a list of column names to explode. This is useful when you have multiple list-like columns that need to be expanded in parallel.





138
pandas
There is an attribute on a dataframe column to check if each row has a unique value:

df['colname'].isunique





139
Azure Functions
Serverless workflow. A series of functions can be chained together, and you can introduce state which makes it possible to devise complex long running workflows via Durable Functions. Another choice for workflows is Logic apps that can monitor external events, perform branching logic and invoke functions as a result.





140
Azure Functions
A trigger is an object that defines a specific function. For example, if you want a function to execute every 10 minutes, you could use a timer trigger.

Every function must have exactly one trigger associated with it. If you want to execute a piece of logic that runs under multiple conditions, you need to create multiple functions that share the same core function code.





141
Azure Functions
Irregular but important business flows. Getting a new customer and onboarding that customer is an example where your code has a good reason to run. Such a flow likely consists of operations like interacting with a data store, sending out emails, and more.





142
Azure Functions
Trigger types and purpose:
Event Hub: Execute a function when an event hub receives a new event
Event Grid:	Execute a function based on Event Grid subscriptions





143
Azure Functions
Reasons to choose WebJobs over Azure Functions

You have specific customizations that you want to make to the JobHost that aren't supported by Azure Functions.
You want to control your app's retry policies.
WebJobs only supports C# on Microsoft Windows.





144
Azure Functions
3. We secured our function against unknown HTTP callers by requiring a function-specific API key be passed with each call. Which of the following fields is the header name in the HTTP requests that needs to contain this key? 

x-functions-key
The API key can be included as a query string parameter named code, or as an HTTP header named x-functions-key.





145
Azure Functions
Bindings comes in two flavors, input, and output. Input bindings can be used to pass data to your function from a data source different than the one that triggered the function.





146
Azure Functions
By default, functions have a timeout of five (5) minutes. This timeout is configurable to a maximum of 10 minutes. If your function requires more than 10 minutes to execute, you can host it on a VM. Additionally, if your service is initiated through an HTTP request and you expect that value as an HTTP response, the timeout is further restricted to 2.5 minutes.





147
Azure Functions
API Management	APIM provides security and routing for your HTTP triggered function endpoints as a way to expose them as a true REST API.





148
Azure Functions 
Most binding types also need a fourth property:

Connection - Provides the name of an app setting key that contains the connection string. Bindings use connection strings stored in app settings to keep secrets out of the function code. Connection strings make your code more configurable and secure.





149
Azure Functions 
Azure Blob Storage - Blob Storage bindings allow you to read from a blob.

Azure Cosmos DB - Azure Cosmos DB input bindings use the SQL API to retrieve one or more Azure Cosmos DB documents, and pass them to the input parameter of the function. The document ID, or query parameters, can be determined based on the trigger that invokes the function.





150
Azure Functions 
Input bindings allow you to connect your function to a data source. You can connect to several types of data sources, and the parameters for each vary. To resolve values from input sources, use binding expressions in the function.json file, in function parameters, or in code.





151
Azure Functions 
Bindings are defined in JSON. A binding is configured in your function's configuration file, which is named function.json and lives in the same folder as your function code.

Let's examine a sample input binding:

    ...
    {
      "name": "headshotBlob",
      "type": "blob",
      "path": "thumbnail-images/{filename}",
      "connection": "HeadshotStorageConnection",
      "direction": "in"
    },
    ...





152
Azure Functions 
Every Azure Function must have exactly one trigger associated with it. If you want to use multiple triggers, you must create multiple functions.





153
Azure Functions 
Three properties are required in all bindings, though you might have to supply more properties based on the type of binding and storage you're using.

Name - Defines the function parameter through which you access the data. For example, in a queue input binding, this property is the name of the function parameter that receives the queue message content.
Type - Identifies the type of binding. For example, the type of data or service you want to interact with.
Direction - Indicates the direction data is flowing. For example, is it an input or output binding?





154
Azure Functions 
A binding expression is specialized text in function.json, function parameters, or code that is evaluated when the function is invoked, to yield a value. For example, if you have a Service Bus Queue binding, you could use a binding expression to obtain the name of the queue from App Settings.





155
Azure Functions 
2. Suppose your Azure Function has a blob trigger associated with it and you want it to execute only when png images are uploaded. Which of the following blob trigger Path values should you use? 

samples-workitems/{name}.png
The Path tells the blob trigger where it should monitor for changes, and if there are any filters applied. Adding a file extension to the Path specifies that uploaded files must have that file extension in order for the trigger to invoke the function.





156
Azure Functions 
Just like the other triggers we've seen so far, you can create a blob trigger in the Azure portal. Inside your Azure function, select Blob trigger from the list of predefined trigger types. Then, enter the logic that you want to execute when a blob is created or updated.

One setting that's important to understand is the Path. The Path tells the blob trigger which blob container to monitor to see if a blob is uploaded or updated. By default, the Path value is:

samples-workitems/{name}





157
Azure Functions 
In an blob input binding definition:
      "path": "thumbnail-images/{filename}",
Provide the path, which specifies the container and the item name that goes in it. The path property is required when using the blob trigger, and should be provided in the style shown here, with curly braces around the filename portion of the path. This syntax creates a binding expression that allows you to reference the blob's name in other bindings, and in your function's code.





158
Diffusers from Hugging Face
Device Management: In torch, models and data are moved to the GPU (CUDA) with .to(torch_device), enabling faster processing by taking advantage of parallel GPU computations.





159
Diffusers from Hugging Face
Text Encoder (CLIPTextModel): Also from transformers, the text encoder turns tokenized text into embeddings that represent the semantic meaning of the prompt, guiding image generation to reflect the prompt.





160
Diffusers from Hugging Face
Batch Size: In torch, batch size (len(prompt)) controls the number of prompts processed together. For single-prompt processing, it’s set to 1, impacting memory and processing speed.





161
Diffusers from Hugging Face
AutoencoderKL (VAE): A part of diffusers, this component compresses images into a latent space for efficient computation and then decodes the generated latent representations back into image space.





162
Diffusers from Hugging Face
Latent Space and Latent Noise: The initial noise, generated with torch.randn, creates latents that represent the starting point in the latent space. This noise is progressively denoised to form the final image.





163
Diffusers from Hugging Face
Tokenizer (CLIPTokenizer): Provided by transformers, it converts input text into token IDs. These numerical representations allow the model to process the prompt and understand its meaning.





164
Diffusers from Hugging Face
Prompt Embedding and Conditioning: transformers’ CLIPTokenizer and CLIPTextModel embed both prompt and “unconditioned” input (empty string), allowing flexible control over how strongly the model adheres to the prompt.





165
Diffusers from Hugging Face
Guidance Scale: A float controlling adherence to the prompt, with higher values enforcing a stronger alignment to the prompt, while lower values allow more creativity in the output.





166
Diffusers from Hugging Face
UNet (UNet2DConditionModel): In diffusers, UNet iteratively removes noise from the latent representation, gradually shaping random noise into an image aligned with the text prompt.





167
Diffusers from Hugging Face
Scheduler (UniPCMultistepScheduler): A diffusers module that controls the denoising steps of the diffusion process, determining how the image progressively refines with each step toward the prompt.





168
PP365
You can set a sandbox, production, or trial (subscription-based) environment in administration mode so that only users with System Administrator or System Customizer security roles will be able to sign in to that environment. Administration mode is useful when you want to make operational changes and not have regular users affect your work, and not have your work affect end users (non-admins).





169
PP365
Workflow in the context of Microsoft Dataverse and Power Apps refers to the automation of business processes and tasks. Workflows are a series of steps and rules that define how data is processed and managed within the system. Workflows can be triggered by events such as record creation, update, or deletion, and can perform actions like sending emails, updating records, creating tasks, and more.





170
PP365
Syntax for filtering a dataframe by a specific column, taking only the rows where that column value includes one of a set of substrings.

# email_usernames = ['lauren.kawamoto', 'nicole.soon', 'darryl.nitta', 'yumi.ueda', 'ty.kubota']
email_pattern = "|".join(email_usernames)
cy_user_df = user_df[user_df['internalemailaddress'].str.contains(email_pattern, case=False, na=False)]





171
PP365
When making the role acc_caseUser, I was having trouble making sure that the users were correctly inheriting the role from the teams I assigned them to. This can be checked by looking at one of the records you're checking access to in the GUI, then choosing (...) > Check access > Pick the desired user





172
PP365
Syntax for using the web api to get specific staff records based on a substring in the name column

FActs_endpoint = "ts_staffs?$filter=contains(ts_name, 'Manu')"
staff_records = make_api_request(FActs_endpoint, method='Get')

ts_staffs: table name (pluralized)
filter=contains(ts_name, 'Manu'): filter on specific field based on containing a substring





173
PP365
Background operations (optional)	Select to disable all asynchronous operations (see Asynchronous service) such as workflows and synchronization with Exchange. Emails will not be sent and server-side synchronization for appointments, contacts, and tasks are disabled. Note: Administration mode must be enabled to disable background operations.





174
PP365
You can call on-demand workflows from inside a business process flow. You can configure this from the new business process flow designer by dragging a workflow component to a process stage or to the Global Workflows section.





175
PP365
To support offline use, Power Apps allows data to be cached locally on the device. Users can create, update, and delete records while offline, and these changes are synchronized with the Dataverse database once the device reconnects to the internet. This must all be configured ahead of time.





176
PP365
In most cases, adding users to an environment only gives users access to the environment itself, not to any resources (apps and data) in the environment. You need to configure access to resources by assigning security roles to users.





177
PP365
To reassign (ownership) of all records for a user, the option is in the admin portal > settings > user > select user > somewhere on that blade.





178
Azure AI Search
Binary Quantization: Binary Quantization converts vector embeddings into binary representations, drastically reducing their size. In Azure AI Search, binary quantization enables faster similarity computations and more efficient storage, enhancing overall search performance.





179
Azure AI Search
Improved Result Diversity: By giving higher scores to results that appear high in any of the combined rankings, Reciprocal Rank Fusion (RRF) reduces bias from single algorithms and increases the diversity and quality of search outcomes, ensuring more comprehensive results for users.





180
Azure AI Search
Storage Efficiency: Vector compression reduces the size of high-dimensional vector embeddings, allowing Azure AI Search to store more vectors with less storage space. This optimization is crucial for handling large datasets efficiently.





181
Azure AI Search
AI-Powered Data Enrichment: "Skills" in Azure AI Search refer to AI functions that enhance and transform data during indexing, such as entity recognition, language detection, and sentiment analysis. These skills enrich the data to improve search relevance.





182
Azure AI Search
Enhanced Semantic Embeddings: By leveraging Matryoshka Representation Learning (MRL), Azure AI Search generates more nuanced semantic embeddings. These embeddings facilitate better context recognition and relevance in search results, leading to more accurate and meaningful information retrieval.





183
Azure AI Search
Comprehensive Data Enhancement: The enrichment pipeline can include various operations such as text extraction, translation, entity recognition, and sentiment analysis. These enhancements improve the quality and relevance of the indexed data, leading to better search outcomes.





184
Azure AI Search
Efficient Text Processing: The Text Split skill breaks down large text documents into smaller, manageable token chunks. This facilitates more efficient indexing and processing within Azure AI Search, ensuring that each segment is handled optimally.





185
Azure AI Search
Structured Data Integration: "Mappings" define how source data fields are aligned with search index fields. Proper mappings ensure that data is correctly structured and accessible, facilitating efficient querying and accurate retrieval within Azure AI Search.





186
Azure AI Search
Advanced Semantic Representations: Text-embedding-3 models convert textual data into dense vector representations, capturing the contextual and semantic nuances of the content. This enables Azure AI Search to perform more sophisticated semantic searches.





187
Azure AI Search
Detailed Diagnostic Insights: Debug Sessions offer access to logs and diagnostic information, enabling the detection of problems such as incorrect data mappings, skill misconfigurations, or performance bottlenecks, thereby improving the overall reliability of the search solution.





188
Azure AI Search
Azure AI Search (formerly known as "Azure Cognitive Search") provides secure information retrieval at scale over user-owned content in traditional and conversational search applications.





189
Kali Linux
The man (manual) command displays the manual pages for other commands, providing detailed information about their usage, options, and functionalities. While some shells offer a help command, it typically provides limited information and is not as comprehensive as man.





190
Kali Linux
Kali Linux is maintained by Offsec, the company responsible for the OSCP (Offsec Security Certified Professional) certification exam. Its first version 1.0.0 "moto" was released in March 2013.





191
Kali Linux
The chmod (change mode) command is used to change the access permissions of files and directories. Permissions determine who can read, write, or execute a file. Changing the owner of a file is done using the chown command, not chmod.





192
Kali Linux
The apt-get update command updates the local package index with the latest information about available packages and their versions from the repositories. In contrast, apt-get upgrade upgrades all the installed packages to their latest versions based on the updated package list.





193
Kali Linux
As of the latest releases, Kali Linux uses XFCE as its default desktop environment. XFCE is chosen for its lightweight nature and speed, making it ideal for performance-sensitive tasks common in cybersecurity work.





194
Kali Linux
The rockyou.txt file is a renowned wordlist frequently used as a default for various password-cracking tools. It is conveniently located in the /usr/share/wordlists/ directory on Kali.

It's a compressed archive containing over 14 million plaintext passwords exposed in the infamous 2009 data breach at the social platform RockYou. This massive password list remains highly valuable for penetration testing efforts despite being over a decade old.





195
Kali Linux
Hydra is a popular password-cracking tool in Kali Linux, designed for conducting rapid dictionary attacks against various protocols to discover weak passwords. Another notable tool for password cracking in Kali is John the Ripper, which focuses on cracking hashed passwords.





196
Kali Linux
The first step is to visit the official Kali Linux website and navigate to the download page. You can choose the platform to install it on, such as virtual machines or a bootable USB drive.

Kali allows you to install it in many different environments, including in the cloud, on arm-based devices such as Raspberry Pi, and even through Windows Subsystem for Linux (WSL).





197
Kali Linux
We’ll show you how to install the open-source code editor code-oss, which was built from Microsoft Visual Studio Code.

Head to the terminal and enter:

sudo apt install code-oss





198
Kali Linux
The /etc/shadow file stores secure user account information, including encrypted passwords. This file is readable only by the root user for security reasons. In contrast, /etc/passwd contains user account details like usernames and user IDs but does not store encrypted passwords.





199
Kali Linux
apt (Advanced Package Tool) is the package manager used in Debian-based distributions, including Kali Linux. It simplifies the process of installing, updating, and removing software packages.





200
sklearn
Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data.





201
sklearn
Neighbors-based classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.





202
sklearn
In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an n-dimensional manifold, or n-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of n-dimensional Euclidean space.





203
sklearn
It is possible and recommended to search the hyper-parameter space for the best cross validation score.

Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:

estimator.get_params()





204
sklearn
The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large numbers of samples and has been used across a large range of application areas in many different fields.





205
sklearn
RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:
A budget can be chosen independent of the number of parameters and possible values.
Adding parameters that do not influence the performance does not decrease efficiency.





206
sklearn
NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in sklearn.metrics.pairwise.





207
sklearn
A Hyperparameter search consists of:

    an estimator (regressor or classifier such as sklearn.svm.SVC());
    a parameter space;
    a method for searching or sampling candidates;
    a cross-validation scheme; and
    a score function.





208
sklearn
The classes in sklearn.neighbors can handle either NumPy arrays or scipy.sparse matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.





209
sklearn
The cv_results_ attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with df = pd.DataFrame(est.cv_results_). The cv_results_ attribute of HalvingGridSearchCV and HalvingRandomSearchCV is similar to that of GridSearchCV and RandomizedSearchCV, with additional information related to the successive halving process.





210
sklearn
When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics.





211
DNS
• DNS Rebinding:
A technique used by attackers to bypass the same-origin policy in browsers, allowing them to interact with private networks by making the DNS response point to a local IP address after an initial resolution to a public IP.





212
DNS
• DNS Tunneling:
A method of encapsulating data within DNS queries and responses to bypass network firewalls and restrictions, often used in data exfiltration or command-and-control channels.





213
DNS
• DNS Forwarding:
The process of forwarding DNS queries from one DNS server to another, often used in corporate networks to ensure that all DNS traffic passes through a specific DNS server.





214
DNS
• DNS Filtering:
A security practice where DNS queries are monitored and filtered to block access to harmful or unauthorized domains.





215
DNS
• DNS Interception:
The DNS Interception feature intercepts and replaces DNS queries matching the configured patterns. You can also allow-list domains. Allow-listed domains always take precedence over the DNS Interception policies. Subdomains of intercepted domains must be explicitly added. They are not intercepted automatically. You must run a caching DNS server to use DNS interception.





216
DNS
• Domain Spoofing:
The act of falsifying DNS records or domain names to make one domain appear as another, often used in phishing or other deceptive practices.





217
DNS
• Transparent DNS Proxy:
A network setup where DNS queries are intercepted and automatically redirected to a designated DNS server without the user’s explicit configuration.





218
DNS
• Security Appliance:
A dedicated hardware device designed to protect a network by filtering traffic, detecting intrusions, or performing other security functions.





219
DNS
• DNS Server:
A server that resolves domain names into IP addresses. Examples include Cloudflare's 1.1.1.1 or Google's 8.8.8.8.





220
DNS
• DNS Filtering and Security Policies:
Advanced configurations and rules applied at the DNS level to enforce security policies, including blocking or redirecting traffic to prevent access to unauthorized or malicious sites.





221
DNS
• DNS Amplification Attack:
A type of DDoS attack that exploits DNS servers by sending small queries with spoofed IP addresses, leading to large responses that overwhelm the target.





222
DNS
• Split-Horizon DNS:
A DNS configuration where different responses are given to queries depending on the source of the query (e.g., internal vs. external network users).





223
sklearn 
The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.

from sklearn.model_selection import cross_val_score
clf = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cross_val_score(clf, X, y, cv=5)
scores





224
sklearn 
KFold: KFold splits data into k consecutive folds (typically 5 or 10) for cross-validation, ensuring each fold is used as both a training and validation set. It’s widely used for balanced datasets, providing reliable estimates of a model’s performance on unseen data.





225
sklearn 
LeavePGroupsOut: This cross-validation technique splits data based on group labels, leaving out all data points with a specific group label in each fold. Useful for datasets with naturally grouped data, where each group should either entirely be in training or test sets.





226
sklearn 
Data leakage occurs when information from the test set inadvertently influences the model during training, leading to overly optimistic performance. Cross-validation methods that respect data structure, like LeavePGroupsOut or TimeSeriesSplit, help prevent this risk.





227
sklearn 
Generalization Error: Generalization error is the measure of a model’s ability to accurately predict outcomes for new, unseen data. Lower generalization error indicates a model that performs well beyond its training data, whereas high error suggests overfitting or poor predictive power.





228
sklearn 
When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.

from sklearn import metrics
scores = cross_val_score(
    clf, X, y, cv=5, scoring='f1_macro')
scores
array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])





229
sklearn 
TimeSeriesSplit: TimeSeriesSplit performs cross-validation on time series data, keeping temporal ordering intact by using only past data to predict future data in each fold. It’s designed for forecasting, where future information should not be included in training.





230
sklearn 
An alternative to cross-validation, bootstrapping involves repeatedly sampling data with replacement and evaluating model performance across these samples. It’s effective for small datasets, providing robust estimates of model variance but may not reflect real test distributions as accurately as cross-validation.





231
sklearn 
LeaveOneOut: This cross-validation technique uses every single observation as its own test set, leaving all other points as the training set. While it provides precise model evaluation, it is computationally expensive, particularly for large datasets.





232
sklearn 
ShuffleSplit: ShuffleSplit randomly splits the data multiple times into training and test sets, allowing control over the number of splits, training/test set sizes, and randomization. It’s versatile and works well for datasets with no inherent ordering or stratification needs.





233
sklearn 
Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.





234
sklearn 
permutation_test_score: This function assesses model performance by computing a baseline distribution of scores via label permutation, creating a null hypothesis benchmark. Comparing the model’s score to this distribution reveals whether its performance is significant or due to random chance.





235
MS Identity Platform
The redirect URIs to use in a desktop application depend on the flow you want to use.

Specify the redirect URI for your app by configuring the platform settings for the app in App registrations in the Microsoft Entra admin center.

For apps that use Web Authentication Manager (WAM), redirect URIs need not be configured in MSAL, but they must be configured in the app registration.





236
MS Identity Platform
OpenID Connect versus SAML: The platform uses both OpenID Connect and SAML to authenticate a user and enable single sign-on. SAML authentication is commonly used with identity providers such as Active Directory Federation Services (AD FS) federated to Microsoft Entra ID, so it's often used in enterprise applications.





237
MS Identity Platform
Resource server - The resource server hosts or provides access to a resource owner's data. Most often, the resource server is a web API fronting a data store. The resource server relies on the authorization server to perform authentication and uses information in bearer tokens issued by the authorization server to grant or deny access to resources.





238
MS Identity Platform
URIs for certain apps:
○ Apps that use embedded browsers: https://login.microsoft.com/common/oauth2/nativeclient (Note: if your app would pop up a window which typically contains no address bar, it is using the "embedded browser")
Apps that would use system browsers: http://loclahost (Note: If your app would bring your system's default browser (such as edge, chrome, firefox, and so on) to visit Microsoft login portal, it is using the "system browser".)





239
MS Identity Platform
Client - The client in an OAuth exchange is the application requesting access to a protected resource. The client could be a web app running on a server, a single-page web app running in a user's web browser, or a web API that calls another web API. You'll often see the client referred to as client application, application, or app.





240
MS Identity Platform
If your desktop or mobile application runs on Windows and on a machine connected to a Windows domain (Active Directory or Microsoft Entra joined) it is possible to use the Integrated Windows Authentication (IWA) to acquire a token silently. No UI is required when using the application.





241
MS Identity Platform
ID token - ID tokens are sent to the client application as part of an OpenID Connect flow. They can be sent alongside or instead of an access token. ID tokens are used by the client to authenticate the user. To learn more about how the Microsoft identity platform issues ID tokens, see ID tokens in the Microsoft identity platform.





242
MS Identity Platform
The parties in an authentication flow use bearer tokens to assure, verify, and authenticate a principal (user, host, or service) and to grant or deny access to protected resources (authorization). Bearer tokens in the Microsoft identity platform are formatted as JSON Web Tokens (JWT).





243
MS Identity Platform
Authorization server - The Microsoft identity platform is the authorization server. Also called an identity provider or IdP, it securely handles the end-user's information, their access, and the trust relationships between the parties in the auth flow. The authorization server issues the security tokens your apps and APIs use for granting, denying, or revoking access to resources (authorization) after the user has signed in (authenticated).





244
MS Identity Platform
Resource owner - The resource owner in an auth flow is usually the application user, or end-user in OAuth terminology. The end-user "owns" the protected resource (their data) which your app accesses on their behalf. The resources owners can grant or deny your app (the client) access to the resources they own. For example, your app might call an external system's API to get a user's email address from their profile on that system.





245
MS Identity Platform
Confidential client applications: Apps in this category include:

Web apps that call a web API
Web APIs that call a web API
Daemon apps, even when implemented as a console service like a Linux daemon or a Windows service





246
MS Identity Platform
Authentication scenarios involve two activities:

Acquiring security tokens for a protected web API: We recommend that you use the Microsoft Authentication Library (MSAL), developed and supported by Microsoft.
Protecting a web API or a web app: One challenge of protecting these resources is validating the security token. On some platforms, Microsoft offers middleware libraries.





247
MS Identity Platform
OAuth versus OpenID Connect: The platform uses OAuth for authorization and OpenID Connect (OIDC) for authentication. OpenID Connect is built on top of OAuth 2.0, so the terminology and flow are similar between the two. You can even both authenticate a user (through OpenID Connect) and get authorization to access a protected resource that the user owns (through OAuth 2.0) in one request.





248
Power BI
Transforming API Data into Usable Tables: API responses often contain nested structures (e.g., records, lists). In Power BI, you use the Expand functionality to convert these structures into tables, making the data easier to manipulate. Data transformation may include renaming columns, changing data types, or removing unnecessary fields.





249
Power BI
If there are data sources being accessed within the partition, and data flowing in from other partitions, the Firewall loses its ability to be the gatekeeper, since the data flowing in could be leaked to one of the internally accessed data sources without it knowing about it. Thus the Firewall prevents a partition that accesses other partitions from being allowed to directly access any data sources.





250
Power BI
In Power Query Diagnostics, 'Diagnose Step' is more useful for getting an insight into what evaluations are happening up to a single step, which can help you identify, up to that step, what performance is like as well as what parts of your query are being performed locally or remotely.





251
Power BI
Creating POST Requests in Power Query: You can modify Power Query M code to create a POST request by specifying the endpoint URL, headers (such as Content-Type: application/json), and request body. The body is structured as JSON using Json.FromValue(), converted to binary with Text.ToBinary(), and passed to the Web.Contents() function.





252
Power BI
The Usage Metrics Report semantic model contains usage data for the last 30 days. It can take up to 24 hours for new usage data to be imported. You can't trigger a manual refresh by using the Power BI user interface.





253
Power BI
A warning with query diagnostics, as mentioned elsewhere, is that you'll see drastically different capabilities depending on the connector. For example, many ODBC based connectors won't have an accurate recording of what query is send to the actual backend system, as Power Query only sees what it sends to the ODBC driver.





254
Power BI
If your organization is using Azure Private Link in Power BI, because client-telemetry is not available the usage metrics reports will only contain Report Open events.





255
Power BI
Power BI and API Calls: Power BI supports REST API calls, allowing data import from web services. The built-in Web connector is used for API integration but primarily supports GET requests. More complex methods like POST require manual M code configuration to handle the request body and headers.





256
Power BI
Certain types of views aren't included in performance measurements. For example, when a user selects a link to a report in an email message, the Report View is accounted for in the report usage but there is no event in the performance metrics.





257
Power BI
Handling Optional API Parameters: For more advanced API requests, parameters such as catalog, calculations, annual averages, and aspects can be added in the payload. These allow for more control over the data returned. Adding these parameters will depend on the API’s available options as per its documentation.





258
Power BI
The improved usage metrics report includes the following report pages:

Report usage Provides information about report views and report viewers, such as how many users viewed the report by date.
Report performance Shows the typical report opening times broken down by consumption method and browser types.
FAQ Provides answers to frequently asked questions, such as What is a "Viewer" and what is a "View"?





259
Power BI
on 9/30, when I was trying to upload a report for HEC, I was noticing that Power BI seemed to go unresponsive and not publish. It turned out that I could publish if I switched from the corp network to the guest network. Why this works is a mystery.





260
Power BI
The SUMMARIZE function in DAX is a powerful function used to create summary tables or groupings of data based on specified columns and expressions. It essentially performs a GROUP BY operation similar to SQL.





261
Power BI
Power BI Web Connector Limitation: The "From Web" connector supports only GET requests by default. To use POST requests or include a body/payload in the request, you must manually edit the Power Query M code. The connector does not provide an interface for specifying request types or payloads in the UI.





262
Azure Kubernetes Learning Path
The Docker software automatically configures a local image registry on your machine. You can view the images in this registry with the docker images command.

Console
docker images





263
Azure Kubernetes Learning Path
docker rmi temp-ubuntu:version-1.0

You can't remove an image if a container is still using the image. The docker rmi command returns an error message, which lists the container that relies on the image.





264
Azure Kubernetes Learning Path
A parent image is a container image from which you create your images.

For example, instead of creating an image from scratch and then installing Ubuntu, we'll use an image already based on Ubuntu. We can even use an image that already has nginx installed. A parent image usually includes a container OS.





265
Azure Kubernetes Learning Path
We use the docker build command to build Docker images. Let's assume we use the Dockerfile definition from earlier to build an image. Here's an example that shows the build command:

Bash
docker build -t temp-ubuntu .





266
Azure Kubernetes Learning Path
A base image is an image that uses the Docker scratch image. The scratch image is an empty container image that doesn't create a filesystem layer. This image assumes that the application you're going to run can directly use the host OS kernel.





267
Azure Kubernetes Learning Path
You can remove an image from the local docker registry with the docker rmi command. This is useful if you need to save space on the container host disk, because container image layers add up to the total space available.

code
docker rmi temp-ubuntu:version-1.0





268
Azure Kubernetes Learning Path
Docker containers running on Linux share the host OS kernel, and don't require a container OS as long as the binary can access the OS kernel directly.

However, Windows containers need a container OS. The container depends on the OS kernel to manage services such as the file system, network management, process scheduling, and memory management.





269
Azure Kubernetes Learning Path
Base images allow us more control over the final image contents. Recall from earlier that an image is immutable; you can only add to an image and not subtract.

On Windows, you can only create container images that are based on Windows base container images. Microsoft provides and services these Windows base container images.





270
Azure Kubernetes Learning Path
The Docker Engine consists of several components configured as a client-server implementation where the client and server run simultaneously on the same host. The client communicates with the server using a REST API, which enables the client to also communicate with a remote server instance.





271
Azure Kubernetes Learning Path
Docker Hub is a Software as a Service (SaaS) Docker container registry. Docker registries are repositories that we use to store and distribute the container images we create. Docker Hub is the default public registry Docker uses for image management.





272
Azure Kubernetes Learning Path
There are two alternatives for Docker client: A command-line application named docker, or a Graphical User Interface (GUI) based application called Docker Desktop. Both the CLI and Docker Desktop interact with a Docker server. The docker commands from the CLI or Docker Desktop use the Docker REST API to send instructions to either a local or remote server and function as the primary interface we use to manage our containers.





273
Azure Kubernetes Learning Path
The Docker server is a daemon named dockerd. The dockerd daemon responds to client requests via the Docker REST API and can interact with other daemons. The Docker server is also responsible for tracking the lifecycle of our containers.





274
Azure Kubernetes Learning Path
A container image is a portable package that contains software. It's this image that, when run, becomes our container. The container is an image's in-memory instance.

A container image is immutable. Once you've built an image, you can't change it. The only way to change an image is to create a new image.





275
Azure Kubernetes Learning Path
A single image can have multiple tags assigned to it. By convention, the most recent version of an image is assigned the latest tag and a tag that describes the image version number. When you release a new version of an image, you can reassign the latest tag to reference the new image.





276
Azure Kubernetes Learning Path
There are several objects that you'll create and configure to support your container deployments. These include networks, storage volumes, plugins, and other service objects. We won't cover all of these objects here, but it's good to keep in mind that these objects are items that we can create and deploy as needed.





277
Azure Kubernetes Learning Path
The ENTRYPOINT in the Dockerfileile indicates which process will execute once we run a container from an image. If there's no ENTRYPOINT or another process to be executed, Docker will interpret that as there's nothing for the container to do, and the container will exit.





278
Azure Kubernetes Learning Path
We use Unionfs to create Docker images. Unionfs is a filesystem that allows you to stack several directories—called branches—in such a way that it appears as if the content is merged. However, the content is physically kept separate. Unionfs allows you to add and remove branches as you build out your file system.





279
Azure Kubernetes Learning Path
Docker images are large files that are initially stored on your PC, and we need tools to manage these files.

The Docker CLI and Docker Desktop allow us to manage images by building, listing, removing, and running them. We manage Docker images by using the docker client. The client doesn't execute the commands directly, and sends all queries to the dockerd daemon.





280
Azure Kubernetes Learning Path
A Dockerfile is a text file that contains the instructions we use to build and run a Docker image. It defines the following aspects of the image:

The base or parent image we use to create the new image
Commands to update the base OS and install additional software
Build artifacts to include, such as a developed application
Services to expose, such as storage and network configuration
Command to run when the container is launched





281
Search Engine Optimization
Answer The Public is a keyword research tool that visualizes search questions and autocomplete suggestions. It helps users generate content ideas by uncovering what questions and topics people are searching for related to specific keywords.





282
Search Engine Optimization
Robots.txt is a text file placed in the root directory of a website that instructs search engine crawlers on which pages or sections to crawl or avoid. It helps manage crawler access, preventing the indexing of certain parts of a website to control search engine visibility.





283
Search Engine Optimization
Google PageSpeed Insights analyzes the content of a web page and provides suggestions to make it faster. It evaluates both desktop and mobile performance, offering recommendations to improve load times and overall website performance for better user experience.





284
Search Engine Optimization
An XML sitemap is a file that lists all the important pages of a website, helping search engines crawl the site more effectively. It improves the discoverability of webpages, ensures all content is indexed, and can include additional metadata about each URL.





285
Search Engine Optimization
Ahrefs is a comprehensive SEO toolset used for backlink analysis, keyword research, competitor analysis, and site audits. It helps users identify backlink opportunities, monitor website performance, and gain insights into competitors' strategies to enhance their own SEO efforts.





286
Search Engine Optimization
Yoast SEO is a WordPress plugin that assists in optimizing website content for better SEO. It provides tools for managing meta tags, generating sitemaps, and improving content readability, helping users enhance their site’s search engine visibility and performance.





287
Search Engine Optimization
SEOquake is a browser extension that provides on-page SEO audits, keyword analysis, and competitive insights directly within the browser. It facilitates quick SEO evaluations without leaving the webpage, offering valuable data for optimizing content and strategies.





288
Search Engine Optimization
On-page SEO involves optimizing individual web pages to rank higher and earn more relevant traffic in search engines. This includes optimizing content, HTML elements, and internal links to improve the page’s visibility and relevance for targeted keywords.





289
Search Engine Optimization
Content quality refers to the overall value and relevance of the information provided on a website. High-quality content engages users, addresses their needs, and is essential for improving SEO rankings by demonstrating authority and expertise to search engines.





290
Search Engine Optimization
Crawl errors are issues that prevent search engine bots from accessing or indexing certain pages on a website. Identifying and fixing these errors improves website visibility in search results by ensuring that all important pages are accessible to search engines.





291
Search Engine Optimization
Bing Webmaster Tools is a free service by Bing that assists webmasters in managing their site’s presence in Bing search results. It offers tools for site scanning, keyword research, backlink analysis, and performance reporting to optimize SEO.





292
Search Engine Optimization
Crawling is the process by which search engine bots systematically browse the web to discover and index content. It is essential for search engines to include webpages in their search results, ensuring that new and updated content is recognized.





293
Search Engine Optimization
SERP (Search Engine Results Page) refers to the page displayed by search engines in response to a user's query. The goal of SEO is to rank higher on SERPs to increase visibility and drive more traffic to a website, thereby enhancing its online presence and reach.





294
Search Engine Optimization
Google Analytics is a free web analytics service that tracks and reports website traffic. It provides insights into user behavior, traffic sources, and conversion metrics, enabling users to understand how visitors interact with their site and measure the effectiveness of their marketing efforts.





295
Search Engine Optimization
Google Search Console is a free service that helps monitor, maintain, and troubleshoot a website's presence in Google Search results. It provides data on search performance, indexing status, and SEO issues, enabling webmasters to optimize their sites effectively.





296
Search Engine Optimization
Keyword density is the percentage of times a keyword appears on a webpage compared to the total number of words. It is used to optimize content for relevant keywords without overstuffing, ensuring that the content remains natural and readable while signaling relevance to search engines.





297
Search Engine Optimization
Domain Authority (DA), developed by Moz, is a metric that predicts how well a website will rank on search engine result pages (SERPs). It is based on various factors, including the quality and quantity of backlinks, and helps gauge a site’s overall SEO strength.





298
Search Engine Optimization
The Mobile-Friendly Test is a tool by Google that assesses how easily a visitor can use a page on a mobile device. It ensures websites are optimized for mobile users, which is a significant ranking factor, as search engines prioritize mobile-friendly sites in their results.





299
Search Engine Optimization
Off-page SEO refers to actions taken outside of your own website to impact your rankings within search engine results. It primarily involves backlink building, social media marketing, and influencer outreach to enhance the site’s authority and reputation.





300
Search Engine Optimization
SEO Spider by Screaming Frog is a website crawling tool that helps identify SEO issues such as broken links, duplicate content, and missing metadata. It assists in conducting comprehensive site audits to improve a website’s SEO performance and search engine rankings.





301
Search Engine Optimization
A meta description is an HTML attribute that provides a brief summary of a webpage’s content. It appears below the title in search results and can influence click-through rates by enticing users with a concise and relevant description of the page.





302
Search Engine Optimization
Google Keyword Planner is a free tool within Google Ads that assists in discovering keywords and providing search volume data. It is used for keyword research and planning SEO strategies by identifying relevant and high-performing keywords to target.





303
Search Engine Optimization
Link equity refers to the value or authority passed from one page to another through hyperlinks. It influences the ranking potential of linked pages, helping them to perform better in search engine results by distributing authority throughout the website.





304
Search Engine Optimization
A website crawler is a tool that systematically browses the web to index and analyze website content. It identifies SEO issues, broken links, and content structure, providing valuable insights for optimization and ensuring that websites are efficiently indexed by search engines.





305
Search Engine Optimization
A backlink is an incoming hyperlink from one webpage to another website. Backlinks are crucial for SEO as they signal authority and trustworthiness to search engines, thereby improving a website’s search engine rankings and visibility.





306
Search Engine Optimization
Link Explorer by Moz is a tool for analyzing a website’s backlink profile and domain authority. It helps users identify linking opportunities, monitor backlink quality, and understand the overall strength of their site’s SEO through detailed backlink data.





307
Search Engine Optimization
Link building is the practice of acquiring backlinks from other websites to improve a site's authority and search rankings. It is critical for enhancing a website’s SEO performance by increasing the number and quality of inbound links from reputable sources.





308
Search Engine Optimization
Duplicate content refers to identical or very similar content appearing on multiple webpages or across different websites. It can negatively impact SEO by causing confusion for search engines about which page to rank, potentially diluting search rankings.





309
Search Engine Optimization
A canonical tag is an HTML element that specifies the preferred version of a webpage to prevent duplicate content issues. By indicating the canonical URL, it helps consolidate ranking signals and ensures search engines index the correct page.





310
Diffusers Library
Gaussian Process
A probabilistic model for defining distributions over functions, often used in the context of continuous random noise. In diffusion, Gaussian processes model how noise is applied over time, treating noise at each step as a sample from a Gaussian distribution for simplicity.





311
Diffusers Library
There are three main components of the library to know about:
The DiffusionPipeline is a high-level end-to-end class designed to rapidly generate samples from pretrained diffusion models for inference.





312
Diffusers Library
Bayesian Inference
The process of updating probabilities based on new data, often used to refine diffusion model outputs. Bayesian inference adjusts prior beliefs about data distribution with observed data, allowing for improved posterior estimates in reverse diffusion steps, enhancing image realism.





313
Diffusers Library
Score-based Models
A type of diffusion model that estimates a "score" or gradient, representing how much to adjust each pixel to reduce noise. These models iteratively improve an image by minimizing the noise, guided by a score function that optimizes image coherence.





314
Diffusers Library
Noise Schedule
A sequence controlling the amount of noise added to images at each diffusion step. This progressive reduction allows the model to learn to denoise by removing specific levels of noise at each step, eventually generating a clear image from the noise.





315
Diffusers Library
Most models take a noisy sample, and each timestep it predicts the noise residual (other models elarn to predict the previous sample directly or hte velcity or v-prediction), the difference between a less noisy image and the input image. You can mix and match models to create other diffusion systems.





316
Diffusers Library
Diffusion Model
A generative model that gradually converts random noise into an image by iteratively refining it through multiple steps, guided by trained patterns learned from data. This is achieved through a process that "diffuses" image details from a noise-laden starting point to a coherent output.





317
Diffusers Library
Latent Space
A compressed, lower-dimensional representation of data where the model learns essential patterns. In image generation, the model manipulates data in this space, using encoded information to generate variations or enhance images, often leading to smoother, more computationally efficient results.





318
Diffusers Library
Markov Chain
A sequence of random variables where each state depends only on the previous one. In diffusion models, the noise addition process is modeled as a Markov chain, allowing controlled transitions from clear images to noisy ones and vice versa, simplifying the math for training.





319
Diffusers Library
Fokker-Planck Equation
A differential equation describing the time evolution of probability distributions in diffusion processes. It provides a mathematical basis for understanding how probabilities spread over time in continuous diffusion, modeling how noise transforms an image’s data distribution.





320
Diffusers Library
UNet Architecture
A neural network architecture designed for image segmentation, commonly used in diffusion models. It employs an encoder-decoder structure, where features are compressed, transformed, and then expanded to refine the image, with skip connections enabling more detailed reconstructions.





321
Diffusers Library
KL Divergence
A measure of difference between two probability distributions. In diffusion models, KL divergence is used to align the generated distribution with the true data distribution, encouraging the model to produce more realistic samples by minimizing the divergence in the reverse process.





322
Diffusers Library
Energy-Based Models (EBMs)
A framework where probability distributions are modeled as energy functions, allowing the model to assign lower energy to more realistic data samples. Diffusion models leverage EBMs to learn a smoother denoising process by associating lower energy states with cleaner images.





323
Diffusers Library
Stochastic Differential Equations (SDEs)
These equations describe how noise evolves over continuous time in diffusion models. By defining the noise dynamics, SDEs allow for modeling both forward (adding noise) and reverse (removing noise) processes as smooth, differential transformations.





324
Diffusers Library
Entropy Regularization
A method that adds a penalty based on entropy to encourage more diverse or structured output in diffusion models. By controlling entropy, the model can better balance between producing highly probable samples and maintaining variability in the generated images.





325
Diffusers Library
Sampling
The method of converting latent representations back into images. Sampling involves running multiple diffusion steps in reverse, gradually removing noise to produce a visually coherent image. Different sampling algorithms can influence generation speed and output quality.





326
Diffusers Library
Unconditional Image Generation: Generate an image from Gaussian noise
    Pipeline: unconditional_image_generation 
Text-Guided Image Generation: Generate an image given a text prompt
    Pipeline: conditional_image_generation





327
Diffusers Library
Annealed Langevin Dynamics
A method used for image generation where noise is gradually reduced while applying denoising steps. Mathematically, it involves adjusting noise strength in each step to maintain a balance between randomness and data fidelity, aiding high-quality sampling in diffusion.





328
Diffusers Library
There are three main components of the library to know about:
Many different schedulers - algorithms that control how noise is added for training, and how to generate denoised images during inference.





329
Diffusers Library
Forward and Reverse Processes
In diffusion, the forward process gradually adds noise to data, following a fixed probabilistic schedule, and the reverse process learns to denoise the noisy data. Mathematically, the forward process models q(x_t | x_{t-1}), while the reverse learns p(x_{t-1} | x_t) to reconstruct images.





330
Diffusers Library
Variational Inference
A technique for approximating complex probability distributions by simpler ones. In diffusion models, variational inference approximates the posterior distribution of the reverse process, enabling computationally feasible training by simplifying the diffusion and denoising steps.





331
Diffusers Library
ELBO (Evidence Lower Bound)
A loss function that diffusion models optimize to maximize the likelihood of generating realistic data. ELBO breaks down the generative process into manageable steps, penalizing differences between real and generated data distributions, balancing reconstruction quality and computational efficiency.





332
Diffusers Library
Classifier-Free Guidance
A technique allowing diffusion models to generate images without an external classifier, enhancing flexibility in output. This guidance uses the model's own internal structure to fine-tune the output, improving realism without the need for additional label constraints.





333
Diffusers Library
There are three main components of the library to know about:
Popular pretrained model architectures and modules that can be used as building blocks for creating diffusion systems.





334
Diffusers Library
Denoising Process
The step-by-step refinement in diffusion models where the model removes noise from a random input, progressing towards an image. At each step, the model estimates the noise in the image and subtracts it, gradually enhancing coherence and detail.





335
Diffusers Library
Text-Guided Image-to-Image Translation: Adapt an image guided by a text prompt
    Pipeline: img2img
Text-Guided Image-Inpainting: Fill the masked part of an image given the image, the mask, and a text prompt
    Pipeline: inpaint





336
Diffusers Library
Likelihood Estimation
The process of calculating the probability of the data under the model's parameters. In diffusion models, maximizing this likelihood means finding parameters that make the observed data most probable, allowing for a better approximation of complex data distributions.





337
Diffusers Library
Reverse Diffusion
The inverse process where an image is generated by removing noise, step-by-step, from an initial noisy state. This approach is essential in diffusion models, reconstructing data by "reversing" the diffusion process and achieving high-quality image outputs.





338
Diffusers Library
Running pip install within a Jupyter notebook is straightforward: you prefix the command with an exclamation mark (!). This tells the notebook to execute the command as a shell command rather than Python code. For example:

python
!pip install ipywidgets

After installing, remember to restart the kernel to ensure the notebook recognizes the new packages.





339
Diffusers Library
Text-Guided Depth-to-Image Translation: adapt parts of an image guided by a text prompt while preserving structure via depth estimation
    pipeline: depth2img

Start by creating an instance of a DiffusionPipeline and specify which pipeline checkpoint you would like to download.





340
Diffusers Library
Score Matching
A method to train models by estimating the gradient of data probability density (the "score") at each noise level. By approximating this gradient, the model learns to shift noisy data towards regions of higher probability, ultimately leading to denoised, realistic images.





341
Python
The first bit of information that you can gather from Response is the status code. A status code informs you of the status of the request.

For example, a 200 OK status means that your request was successful, whereas a 404 NOT FOUND status means that the resource you were looking for wasn’t found. There are many other possible status codes as well to give you specific insights into what happened with your request.





342
Python
The purpose of the .__init__() method in a Python class is to initialize instance attributes.

When you create an instance of a class, Python automatically calls .__init__() using the same arguments that you’ve passed to the constructor. This method sets up the initial state of the object by assigning values to instance attributes.





343
Python
PyTorch is based on Torch, a framework for doing fast computation that is written in C. Torch has a Lua wrapper for constructing models.

PyTorch wraps the same C back end in a Python interface. But it’s more than just a wrapper. Developers built it from the ground up to make models easy to write for Python programmers.





344
Python
Named replacement fields allow you to insert values from a dictionary into a string by using keys. This method is more readable and intuitive than using positional arguments with tuples.





345
Python
When it comes to pre-trained models, TensorFlow Hub steps in with models that you can use right away, like ResNet and MobileNet. You can save time by using these models to recognize patterns or objects in images so you don’t have to start from scratch.





346
Python
To customize headers, you pass a dictionary of HTTP headers to get() using the headers parameter. For example, you can change your previous search request to highlight matching search terms in the results by specifying the text-match media type in the Accept header:

import requests

response = requests.get(
    "https://api.github.com/search/repositories",
    params={"q": '"real python"'},
    headers={"Accept": "application/vnd.github.text-match+json"},
)





347
Python
You don’t need to use a dedicated tool to create a README file for your project. You can use any plain text editor for this task. However, there are many tools available that can help automate the README creation process. You can read more about this in the section on tools for README creation.





348
Python
Python sequences have three common characteristics:

  They are iterable, which means you can iterate through them.
  They have a length, which means you can pass them to len() to get the number of elements it contains.
  An element of a sequence can be accessed based on its position in the sequence using an integer index.





349
Python
Augmented assignment operators provide a more concise way to update the value of a variable. For example, counter += 1 is a shorthand for counter = counter + 1.

These operators rely on special methods like .__iadd__() for in-place operations. If the data type is mutable, then Python performs the operation in place. If the data type is immutable, Python returns a new object with the updated value.





350
Python
The re.sub function replaces all occurrences of a specified pattern in a string with a given replacement by default. If you want to replace only the first instance, you can use the count parameter and set it to 1.





351
Python
Magic methods, also known as special methods or dunder methods, are methods in Python whose names start and end with a double underscore.

These methods have special meanings and are automatically called by Python in response to certain operations, such as instantiation, sequence indexing, and attribute management.





352
Python
Many popular machine learning algorithms and datasets are built into TensorFlow and are ready to use. In addition to the built-in datasets, you can access Google Research Datasets or use Google’s Dataset Search to find even more.





353
Python
Note: Requests uses a package called certifi to provide certificate authorities. This lets Requests know which authorities it can trust. Therefore, you should update certifi frequently to keep your connections as secure as possible.





354
Python
The CRUD operations correspond almost one-to-one with SQL commands:

Create: INSERT
Read: SELECT
Update: UPDATE
Delete: DELETE
So, to modify existing data in SQL, you would use the UPDATE command.





355
Python
The common practice is to use lists for homogeneous objects and tuples for heterogeneous objects. Lists are typically used when all elements are of the same type, while tuples are used when elements are of different types.





356
Python
In Python’s unittest package, you can use fixtures to set up and tear down test environments for each test method. To do this, you can use the setUp() and tearDown() method, respectively. These methods are automatically called before and after each test method.





357
Python
When you try to create an instance of a class that inherits from the Sequence abstract base class, but doesn’t implement .__getitem__() and .__len__(), then Python raises a TypeError:

The Sequence abstract base class requires that any class that inherits from it must implement the .__getitem__() and .__len__() methods.





358
Python
.headers returns a dictionary-like object, allowing you to access header values by key. For example, to see the content type of the response payload, you can access "Content-Type":

>>> response.headers["Content-Type"]
'application/json; charset=utf-8'

There’s something special about this dictionary-like headers object, though. The HTTP specification defines headers as case-insensitive, which means that you’re able to access these headers without worrying about their capitalization:





359
Python
TorchText is part of the PyTorch ecosystem and is designed to handle text data for natural language processing tasks. It offers tools for managing data loading, applying text transformations, and integrating with datasets.

This library simplifies tasks like text classification and language modeling by making vocabulary creation and tokenization more straightforward.





360
Python
In Python, you can compare sequences of the same type. When comparing two sequences, the first 
non-equal
 value determines the outcome.

For example, consider the following lists:

numbers = [1, 2, 3, 4, 5]
more_numbers = [1, 2, 2, 10, 20]
When you compare these lists, you’ll find that numbers is greater than more_numbers.





361
Python
When a request fails, you may want your application to retry the same request. However, Requests won’t do this for you by default. To apply this functionality, you need to implement a custom transport adapter.

Transport adapters let you define a set of configurations for each service that you’re interacting with.





362
Python
More coding languages are supported in TensorFlow than in PyTorch, which has a C++ API. You can use TensorFlow in both JavaScript and Swift. If you don’t want to write much low-level code, then Keras abstracts away a lot of the details for common use cases so you can build TensorFlow models without sweating the details.





363
Python
The response headers can give you useful information, such as the content type of the response payload and a time limit on how long to cache the response. To view these headers, access .headers:

>>> import requests

>>> response = requests.get("https://api.github.com")
>>> response.headers
{'Server': 'GitHub.com',
...
'X-GitHub-Request-Id': 'AE83:3F40:2151C46:438A840:65C38178'}





364
Python
The load_tests() function in Python’s unittest framework is a hook for customizing test loading and suite creation. It takes three arguments: a test loader, a series of tests, and a test discovery pattern. When you call unittest.main(), load_tests() gets called automatically.





365
Python
Authentication helps a service understand who you are. Typically, you provide your credentials to a server by passing data through the Authorization header or a custom header defined by the service. All the functions of Requests that you’ve seen to this point provide a parameter called auth, which allows you to pass your credentials:

>>> import requests

>>> response = requests.get(
...     "https://httpbin.org/basic-auth/user/passwd",
...     auth=("user", "passwd")
... )





366
Python
In Python, special methods support comparison operators.

For example, the .__lt__() method supports the < operator, .__gt__() supports >, and the .__eq__() method supports the == operator.

These methods allow you to define custom behavior for comparison operations in your custom classes.





367
Python
If you invoke Response.raise_for_status(), then Requests will raise an HTTPError for status codes between 400 and 600. If the status code indicates a successful request, then the program will proceed without raising that exception.





368
Python
According to the HTTP specification, POST, PUT, and the less common PATCH requests pass their data through the message body rather than through parameters in the query string. Using Requests, you’ll pass the payload to the corresponding function’s data parameter.

data takes a dictionary, a list of tuples, bytes, or a file-like object.





369
Python
To create an iterator in Python, you need to implement .__iter__() and .__next__().

The .__iter__() special method initializes the iterator and must return an iterator object, typically self.

The .__next__() method retrieves the next value in the data stream and must raise a StopIteration exception when the data stream is exhausted.





370
Python
To make a class iterable, you need to define the .__iter__() special method in the class. This method must return an iterator, which Python uses in its iteration protocol.





371
Python
import re

def replace_punctuations_and_spaces(input_string):
    # Define the pattern to match all punctuations and spaces
    pattern = r'[^\w]'
    # Replace all matched characters with an empty string
    result = re.sub(pattern, '', input_string)
    return result





372
Python
Because the decoding of bytes to a str requires an encoding scheme, Requests will try to guess the encoding based on the response’s headers if you don’t specify one. You can provide an explicit encoding by setting .encoding before accessing .text:

>>> response.encoding = "utf-8"  # Optional: Requests infers this.
>>> response.text
'{"current_user_url":"https://api.github.com/user", ...}'





373
Python
Sessions are used to persist parameters across requests. For example, if you want to use the same authentication across multiple requests, then you can use a session:

import requests

TOKEN = "<YOUR_GITHUB_PA_TOKEN>"

with requests.Session() as session:
    session.auth = TokenAuth(TOKEN)

    first_response = session.get("https://api.github.com/user")
    second_response = session.get("https://api.github.com/user")





374
Python
With TensorFlow’s TF Image, part of TensorFlow’s core, you get all the basics for resizing, cropping, and flipping images to get them ready for your models. Then there’s TensorFlow Datasets (TFDS), which offers a huge selection of real-world image datasets that are easy to plug into your projects.





375
Python
When you make a request, the Requests library prepares the request before actually sending it to the destination server. Request preparation includes things like validating headers and serializing JSON content.

You can view the PreparedRequest object by accessing .request on a Response object: