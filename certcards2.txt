





1
DNS
• Domain Spoofing:
The act of falsifying DNS records or domain names to make one domain appear as another, often used in phishing or other deceptive practices.





2
DNS
• DNS Tunneling:
A method of encapsulating data within DNS queries and responses to bypass network firewalls and restrictions, often used in data exfiltration or command-and-control channels.





3
DNS
• DNS Filtering:
A security practice where DNS queries are monitored and filtered to block access to harmful or unauthorized domains.





4
DNS
• DNS Forwarding:
The process of forwarding DNS queries from one DNS server to another, often used in corporate networks to ensure that all DNS traffic passes through a specific DNS server.





5
DNS
• Transparent DNS Proxy:
A network setup where DNS queries are intercepted and automatically redirected to a designated DNS server without the user’s explicit configuration.





6
DNS
• DNS Interception:
The DNS Interception feature intercepts and replaces DNS queries matching the configured patterns. You can also allow-list domains. Allow-listed domains always take precedence over the DNS Interception policies. Subdomains of intercepted domains must be explicitly added. They are not intercepted automatically. You must run a caching DNS server to use DNS interception.





7
DNS
• Security Appliance:
A dedicated hardware device designed to protect a network by filtering traffic, detecting intrusions, or performing other security functions.





8
DNS
• DNS Server:
A server that resolves domain names into IP addresses. Examples include Cloudflare's 1.1.1.1 or Google's 8.8.8.8.





9
DNS
• DNS Filtering and Security Policies:
Advanced configurations and rules applied at the DNS level to enforce security policies, including blocking or redirecting traffic to prevent access to unauthorized or malicious sites.





10
DNS
• Split-Horizon DNS:
A DNS configuration where different responses are given to queries depending on the source of the query (e.g., internal vs. external network users).





11
DNS
• DNS Amplification Attack:
A type of DDoS attack that exploits DNS servers by sending small queries with spoofed IP addresses, leading to large responses that overwhelm the target.





12
DNS
Group Policy Management for DNS
Using Group Policy in Windows to enforce DNS settings and configurations across multiple servers and clients. It enables centralized management of DNS policies, security settings, and zone configurations, ensuring consistency and compliance.





13
DNS
Split-Brain DNS
A DNS configuration strategy where different DNS records are served based on the client's network location. Internal users receive private IP addresses, while external users receive public-facing addresses for the same domain, enhancing security and functionality.





14
DNS
Conditional Forwarders vs. DNS Policies
Conditional Forwarders direct specific domain queries to designated DNS servers, while DNS Policies allow for more granular control based on various criteria like client IP ranges. Both enhance DNS management but serve different operational needs.





15
DNS
Reverse Lookup Zones
DNS zones that translate IP addresses back to hostnames. They are used for reverse DNS lookups, allowing clients to determine the hostname associated with a given IP address, which is useful for troubleshooting and security.





16
DNS
TSIG (Transaction SIGnature)
A security mechanism used to authenticate DNS messages, ensuring that zone transfers and other DNS communications are from trusted sources. TSIG uses shared secret keys and cryptographic signatures to verify the authenticity of DNS transactions.





17
DNS
Refresh Interval
The period following the No-Refresh Interval during which the timestamp on a DNS record can be updated. If a record is refreshed within this interval, it remains active; otherwise, it becomes eligible for scavenging.





18
DNS
Zone Transfers
The process of copying DNS zone data from a primary DNS server to secondary DNS servers. Zone transfers ensure DNS data consistency and redundancy across multiple DNS servers, enhancing reliability and load distribution.





19
DNS
Conditional Forwarding
A DNS configuration that directs queries for specific domain names to designated DNS servers instead of using default forwarders. It's useful for managing inter-office DNS resolution or directing queries for partner domains to their respective DNS servers.





20
DNS
DNS Manager
A graphical administrative tool in Windows Server used to manage DNS settings and zones. DNS Manager provides interfaces for creating and modifying DNS records, configuring zone properties, setting up forwarders, and implementing security measures.





21
DNS
DNS Scavenging
An automated process in Windows DNS that removes stale or outdated DNS records. It helps maintain DNS integrity by deleting records no longer valid, such as those from devices removed from the network or with changed IP addresses.





22
DNS
TSIG (Transaction SIGnature)
A security protocol used to authenticate DNS zone transfers between servers. TSIG employs shared secret keys and cryptographic signatures to ensure that only authorized servers can perform zone transfers, enhancing DNS security.





23
DNS
Remove-DnsServerZone (PowerShell Cmdlet)
A PowerShell command used to delete an existing DNS zone from a Windows DNS Server. It allows administrators to remove obsolete or unnecessary zones efficiently through scripts or command-line operations.





24
DNS
IPsec (Internet Protocol Security)
A suite of protocols used to secure Internet Protocol (IP) communications by authenticating and encrypting each IP packet in a communication session. In DNS, IPsec can protect zone transfers by ensuring data integrity and confidentiality.





25
DNS
Forward Lookup Zones
DNS zones that translate hostnames to IP addresses. They are the most common type of DNS zones, enabling clients to resolve domain names to their corresponding IPv4 or IPv6 addresses.





26
DNS
SRV (Service) Records
DNS records that specify the location (hostname and port) of servers for particular services within a domain. Essential for services like SIP (VoIP) or LDAP, they enable clients to discover service endpoints automatically.





27
DNS
DNS Policies
A feature in Windows DNS Server that allows administrators to define rules for how DNS queries are handled. Policies can filter queries or provide different responses based on criteria like client IP addresses, enabling scenarios like split-brain DNS.





28
DNS
Root Hints
A list of authoritative DNS servers for the root zone. Root hints are used by DNS servers to initiate recursive queries when resolving domain names, especially when no forwarders are configured.





29
DNS
Dynamic Updates
A DNS feature that allows DNS records to be automatically updated in response to changes in IP addresses or other network configurations. Dynamic updates simplify DNS management by reducing the need for manual record adjustments.





30
DNS
DNS Forwarders
DNS servers configured to forward queries they cannot resolve locally to external DNS servers. Forwarders help improve DNS resolution efficiency and reduce latency by directing unresolved queries to designated upstream servers.





31
DNS
No-Refresh Interval
In DNS scavenging, the period during which the timestamp on a DNS record cannot be updated. It prevents excessive DNS updates, helping determine when a record becomes stale and eligible for removal during scavenging.





32
DNS
Add-DnsServerResourceRecordA (PowerShell Cmdlet)
A PowerShell command used to create a new A (Address) record in a Windows DNS Server. It maps a hostname to its corresponding IPv4 address, enabling automated and script-based DNS record management.





33
DNS
Stub Zone
A DNS zone that contains only essential information about another zone's authoritative DNS servers. It facilitates efficient DNS resolution by allowing a DNS server to know where to forward queries for that specific zone without storing all DNS records.





34
DNS
DNS Zone Types
Different types of DNS zones include Primary, Secondary, Stub, and Active Directory-Integrated zones. Each type serves distinct roles in DNS management, such as data storage, redundancy, delegation, and integration with directory services.





35
DNS
Active Directory-Integrated Zones
DNS zones that store their data within the Active Directory database. This integration allows for secure dynamic updates and automatic replication of DNS data across all domain controllers, enhancing security and redundancy.





36
DNS
Securing Zone Transfers
Measures taken to protect DNS zone data during transfers between DNS servers. This includes restricting transfers to authorized IP addresses and using security protocols like TSIG or IPsec to authenticate and encrypt the data, ensuring integrity and confidentiality.





37
sklearn
RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:
A budget can be chosen independent of the number of parameters and possible values.
Adding parameters that do not influence the performance does not decrease efficiency.





38
sklearn
Neighbors-based classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.





39
sklearn
It is possible and recommended to search the hyper-parameter space for the best cross validation score.

Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:

estimator.get_params()





40
sklearn
LeavePGroupsOut: This cross-validation technique splits data based on group labels, leaving out all data points with a specific group label in each fold. Useful for datasets with naturally grouped data, where each group should either entirely be in training or test sets.





41
sklearn
In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an n-dimensional manifold, or n-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of n-dimensional Euclidean space.





42
sklearn
ShuffleSplit: ShuffleSplit randomly splits the data multiple times into training and test sets, allowing control over the number of splits, training/test set sizes, and randomization. It’s versatile and works well for datasets with no inherent ordering or stratification needs.





43
sklearn
An alternative to cross-validation, bootstrapping involves repeatedly sampling data with replacement and evaluating model performance across these samples. It’s effective for small datasets, providing robust estimates of model variance but may not reflect real test distributions as accurately as cross-validation.





44
sklearn
TimeSeriesSplit: TimeSeriesSplit performs cross-validation on time series data, keeping temporal ordering intact by using only past data to predict future data in each fold. It’s designed for forecasting, where future information should not be included in training.





45
sklearn
The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large numbers of samples and has been used across a large range of application areas in many different fields.





46
sklearn
Generalization Error: Generalization error is the measure of a model’s ability to accurately predict outcomes for new, unseen data. Lower generalization error indicates a model that performs well beyond its training data, whereas high error suggests overfitting or poor predictive power.





47
sklearn
LeaveOneOut: This cross-validation technique uses every single observation as its own test set, leaving all other points as the training set. While it provides precise model evaluation, it is computationally expensive, particularly for large datasets.





48
sklearn
Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.





49
sklearn
NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in sklearn.metrics.pairwise.





50
sklearn
When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.

from sklearn import metrics
scores = cross_val_score(
    clf, X, y, cv=5, scoring='f1_macro')
scores
array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])





51
sklearn
Data leakage occurs when information from the test set inadvertently influences the model during training, leading to overly optimistic performance. Cross-validation methods that respect data structure, like LeavePGroupsOut or TimeSeriesSplit, help prevent this risk.





52
sklearn
permutation_test_score: This function assesses model performance by computing a baseline distribution of scores via label permutation, creating a null hypothesis benchmark. Comparing the model’s score to this distribution reveals whether its performance is significant or due to random chance.





53
sklearn
A Hyperparameter search consists of:

    an estimator (regressor or classifier such as sklearn.svm.SVC());
    a parameter space;
    a method for searching or sampling candidates;
    a cross-validation scheme; and
    a score function.





54
sklearn
The classes in sklearn.neighbors can handle either NumPy arrays or scipy.sparse matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.





55
sklearn
The cv_results_ attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with df = pd.DataFrame(est.cv_results_). The cv_results_ attribute of HalvingGridSearchCV and HalvingRandomSearchCV is similar to that of GridSearchCV and RandomizedSearchCV, with additional information related to the successive halving process.





56
sklearn
When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics.





57
Azure Kubernetes Learning Path
The Docker software automatically configures a local image registry on your machine. You can view the images in this registry with the docker images command.

Console
docker images





58
Azure Kubernetes Learning Path
We use the docker build command to build Docker images. Let's assume we use the Dockerfile definition from earlier to build an image. Here's an example that shows the build command:

Bash
docker build -t temp-ubuntu .





59
Azure Kubernetes Learning Path
Base images allow us more control over the final image contents. Recall from earlier that an image is immutable; you can only add to an image and not subtract.

On Windows, you can only create container images that are based on Windows base container images. Microsoft provides and services these Windows base container images.





60
Azure Kubernetes Learning Path
docker rmi temp-ubuntu:version-1.0

You can't remove an image if a container is still using the image. The docker rmi command returns an error message, which lists the container that relies on the image.





61
Azure Kubernetes Learning Path
A parent image is a container image from which you create your images.

For example, instead of creating an image from scratch and then installing Ubuntu, we'll use an image already based on Ubuntu. We can even use an image that already has nginx installed. A parent image usually includes a container OS.





62
Azure Kubernetes Learning Path
There are several objects that you'll create and configure to support your container deployments. These include networks, storage volumes, plugins, and other service objects. We won't cover all of these objects here, but it's good to keep in mind that these objects are items that we can create and deploy as needed.





63
Azure Kubernetes Learning Path
Docker containers running on Linux share the host OS kernel, and don't require a container OS as long as the binary can access the OS kernel directly.

However, Windows containers need a container OS. The container depends on the OS kernel to manage services such as the file system, network management, process scheduling, and memory management.





64
Azure Kubernetes Learning Path
Docker Hub is a Software as a Service (SaaS) Docker container registry. Docker registries are repositories that we use to store and distribute the container images we create. Docker Hub is the default public registry Docker uses for image management.





65
Azure Kubernetes Learning Path
A base image is an image that uses the Docker scratch image. The scratch image is an empty container image that doesn't create a filesystem layer. This image assumes that the application you're going to run can directly use the host OS kernel.





66
Azure Kubernetes Learning Path
There are two alternatives for Docker client: A command-line application named docker, or a Graphical User Interface (GUI) based application called Docker Desktop. Both the CLI and Docker Desktop interact with a Docker server. The docker commands from the CLI or Docker Desktop use the Docker REST API to send instructions to either a local or remote server and function as the primary interface we use to manage our containers.





67
Azure Kubernetes Learning Path
You can remove an image from the local docker registry with the docker rmi command. This is useful if you need to save space on the container host disk, because container image layers add up to the total space available.

code
docker rmi temp-ubuntu:version-1.0





68
Azure Kubernetes Learning Path
A container image is a portable package that contains software. It's this image that, when run, becomes our container. The container is an image's in-memory instance.

A container image is immutable. Once you've built an image, you can't change it. The only way to change an image is to create a new image.





69
Azure Kubernetes Learning Path
The Docker Engine consists of several components configured as a client-server implementation where the client and server run simultaneously on the same host. The client communicates with the server using a REST API, which enables the client to also communicate with a remote server instance.





70
Azure Kubernetes Learning Path
The ENTRYPOINT in the Dockerfileile indicates which process will execute once we run a container from an image. If there's no ENTRYPOINT or another process to be executed, Docker will interpret that as there's nothing for the container to do, and the container will exit.





71
Azure Kubernetes Learning Path
The Docker server is a daemon named dockerd. The dockerd daemon responds to client requests via the Docker REST API and can interact with other daemons. The Docker server is also responsible for tracking the lifecycle of our containers.





72
Azure Kubernetes Learning Path
We use Unionfs to create Docker images. Unionfs is a filesystem that allows you to stack several directories—called branches—in such a way that it appears as if the content is merged. However, the content is physically kept separate. Unionfs allows you to add and remove branches as you build out your file system.





73
Azure Kubernetes Learning Path
A single image can have multiple tags assigned to it. By convention, the most recent version of an image is assigned the latest tag and a tag that describes the image version number. When you release a new version of an image, you can reassign the latest tag to reference the new image.





74
Azure Kubernetes Learning Path
Docker images are large files that are initially stored on your PC, and we need tools to manage these files.

The Docker CLI and Docker Desktop allow us to manage images by building, listing, removing, and running them. We manage Docker images by using the docker client. The client doesn't execute the commands directly, and sends all queries to the dockerd daemon.





75
Azure Kubernetes Learning Path
For more advanced scenarios, both Linux and Windows provide additional network options. For example, the overlay option creates a virtual switch from the host network, so containers on that network can get IP addresses from DHCP servers or operate with IP addresses from that network segment. Furthermore, Docker allows third-party vendors to create network plugins.





76
Azure Kubernetes Learning Path
To place a container in the run state, use the run command. You can also restart a container that's already running. When restarting a container, the container receives a termination signal to enable any running processes to shut down gracefully before the container's kernel terminates.





77
Azure Kubernetes Learning Path
The bridge network is the default configuration applied to containers when launched without specifying any other network configuration. This network is an internal, private network used by the container, and it isolates the container network from the Docker host network.





78
Azure Kubernetes Learning Path
If you need to terminate the container, use the kill command to send a kill signal. The container's kernel captures the kill signal, but the running process doesn't. This command forcefully terminates the working process in the container.

Lastly, to remove containers that are in a stopped state, use the remove command. After removing a container, all data stored in the container gets destroyed.





79
Azure Kubernetes Learning Path
A volume is stored on the host filesystem at a specific folder location. Choose a folder where you know the data won't be modified by non-Docker processes.

Docker creates and manages the new volume by running the docker volume create command. This command can form part of our Dockerfile definition, which means that you can create volumes as part of the container-creation process. Docker creates the volume if it doesn't exist when you try to mount the volume into a container the first time.





80
Azure Kubernetes Learning Path
Docker containers are the default container architecture the Azure containerization services use, and many other cloud platforms also support them.

For instance, you can deploy Docker containers to Azure Container Instances, Azure App Service, and Azure Kubernetes Services. Each of these options provides you with different features and capabilities.





81
Azure Kubernetes Learning Path
Docker provides us with the docker stats command. This command returns information for the container such as percentage CPU usage, percentage memory usage, I/O written to disk, network data sent and received, and process IDs assigned. This information is useful as an immediate data stream; however, no aggregation is done, because the data isn't stored. We'd have to install third-party software for meaningful data capture over a period of time.





82
Azure Kubernetes Learning Path
Docker containers provide security features to run multiple containers simultaneously on the same host without affecting each other. As we learned, we can configure both data storage and network configuration to isolate our containers or share data and connectivity between specific containers.





83
Azure Kubernetes Learning Path
To pause a running container, use the pause command. This command suspends all processes in the container.

To stop a running container, use the stop command. The stop command enables the working process to shut down gracefully by sending it a termination signal. The container's kernel terminates after the process shuts down.





84
Azure Kubernetes Learning Path
Docker images are stored and made available in registries. A registry is a web service to which Docker can connect to upload and download container images. The most well-known registry is Docker Hub, which is a public registry. Many individuals and organizations publish images to Docker Hub, and you can download and run these images using Docker running on your desktop, on a server, or in the cloud.





85
Azure Kubernetes Learning Path
A registry is organized as a series of repositories. Each repository contains multiple Docker images that share a common name and generally the same purpose and functionality. These images normally have different versions, identified with a tag. This mechanism enables you to publish and retain multiple versions of images for compatibility reasons.





86
Azure Kubernetes Learning Path
The host network enables you to run the container on the host network directly. This configuration effectively removes the isolation between the host and the container at a network level.

Keep in mind that the container can use only ports the host isn't already using.





87
Azure Kubernetes Learning Path
1. A container is launched using the --publish 8080:80 flag. Which of the following options is the most likely network configuration used for the container? 
The Bridge network configuration is an internal, private network used by the container and isolates the container network from the Docker host network. We use the publish flag to map ports between the container and host ports.





88
Azure Kubernetes Learning Path
Container storage drives are less performant.

Containers implement a storage driver to allow your apps to write data. This driver introduces an extra abstraction to communicate with the host OS kernel, and is less performant than writing directly to a host filesystem.

Containers can make use of two options to persist data. The first option is to make use of volumes, and the second is bind mounts.





89
Azure Kubernetes Learning Path
2. Which storage option is the best choice that allows the host and container to share a file to manage name server resolution; for example, the resolve.conf file on Linux? 
A bind mount, like a volume, is stored on the host filesystem at a specific folder location. However, the host is expected to update bind mount data. The resolve.conf contents is expected to change by the host and used by both the container and host.





90
Azure Kubernetes Learning Path
You can remove an image from the local computer with the docker image rm command. Specify the image ID of the image to remove. The following example removes the image for the sample web app.

Bash
docker image rm mcr.microsoft.com/dotnet/core/samples:aspnetapp





91
Azure Kubernetes Learning Path
docker ps is a shortcut for docker container ls. The names of these commands are based on the Linux utilities ps and ls, which list running processes and files, respectively.





92
Azure Kubernetes Learning Path
The docker build command creates a new image by running a Dockerfile. This command's syntax has several parameters:

The -f flag indicates the name of the Dockerfile to use.
The -t flag specifies the name of the image to be created; in this example, myapp:v1.
The final parameter, ., provides the build context for the source files for the COPY command: the set of files on the host computer needed during the build process.





93
Azure Kubernetes Learning Path
Command in DockerFile
ENTRYPOINT	Specifies the operation the container should run when it starts. In this example, it runs the newly built app. You specify the command you want to run and each of its arguments as a string array.





94
Azure Kubernetes Learning Path
Enter the following code to start the sample app. The -d flag is to run it as a background, non-interactive app. The -p flag is to map port 8080 in the container that's created to port 8080 locally. This setting is intended to avoid conflicts with any web apps already running on your computer. The command will respond with a lengthy hexadecimal identifier for the instance.

Bash
docker run -d -p 8080:8080 mcr.microsoft.com/dotnet/samples:aspnetapp





95
Azure Kubernetes Learning Path
By default, Docker doesn't allow inbound network requests to reach your container. You need to tell Docker to assign a specific port number from your computer to a specific port number in the container by adding the -p option to docker run. This instruction enables network requests to the container on the specified port.





96
Azure Kubernetes Learning Path
Dockerfile Example:
FROM mcr.microsoft.com/dotnet/sdk:6.0
WORKDIR /app
COPY myapp_code .
RUN dotnet build -c Release -o /rel
EXPOSE 80
WORKDIR /rel
ENTRYPOINT ["dotnet", "myapp.dll"]





97
Azure Kubernetes Learning Path
Command in DockerFile
RUN	Executes a command in the container. Arguments to the RUN command are command-line commands.





98
Azure Kubernetes Learning Path
You use the docker pull command with the image name to retrieve an image. By default, Docker will download the image tagged latest from that repository on Docker Hub if you specify only the repository name. Keep in mind that you can modify the command to pull different tags and from different repositories.





99
Azure Kubernetes Learning Path
To create a Docker image containing your application, you typically begin by identifying a base image, to which you add files and configuration information. The process of identifying a suitable base image usually starts with an image search on Docker Hub. You want an image that already contains an application framework and all the utilities and tools of a Linux distribution, like Ubuntu or Alpine.





100
Azure Functions
Serverless workflow. A series of functions can be chained together, and you can introduce state which makes it possible to devise complex long running workflows via Durable Functions. Another choice for workflows is Logic apps that can monitor external events, perform branching logic and invoke functions as a result.





101
Azure Functions
Most binding types also need a fourth property:

Connection - Provides the name of an app setting key that contains the connection string. Bindings use connection strings stored in app settings to keep secrets out of the function code. Connection strings make your code more configurable and secure.





102
Azure Functions
Irregular but important business flows. Getting a new customer and onboarding that customer is an example where your code has a good reason to run. Such a flow likely consists of operations like interacting with a data store, sending out emails, and more.





103
Azure Functions
Input bindings allow you to connect your function to a data source. You can connect to several types of data sources, and the parameters for each vary. To resolve values from input sources, use binding expressions in the function.json file, in function parameters, or in code.





104
Azure Functions
A trigger is an object that defines a specific function. For example, if you want a function to execute every 10 minutes, you could use a timer trigger.

Every function must have exactly one trigger associated with it. If you want to execute a piece of logic that runs under multiple conditions, you need to create multiple functions that share the same core function code.





105
Azure Functions
Bindings are defined in JSON. A binding is configured in your function's configuration file, which is named function.json and lives in the same folder as your function code.

Let's examine a sample input binding:

    ...
    {
      "name": "headshotBlob",
      "type": "blob",
      "path": "thumbnail-images/{filename}",
      "connection": "HeadshotStorageConnection",
      "direction": "in"
    },
    ...





106
Azure Functions
Trigger types and purpose:
Event Hub: Execute a function when an event hub receives a new event
Event Grid:	Execute a function based on Event Grid subscriptions





107
Azure Functions
3. We secured our function against unknown HTTP callers by requiring a function-specific API key be passed with each call. Which of the following fields is the header name in the HTTP requests that needs to contain this key? 

x-functions-key
The API key can be included as a query string parameter named code, or as an HTTP header named x-functions-key.





108
Azure Functions
A binding expression is specialized text in function.json, function parameters, or code that is evaluated when the function is invoked, to yield a value. For example, if you have a Service Bus Queue binding, you could use a binding expression to obtain the name of the queue from App Settings.





109
Azure Functions
Every Azure Function must have exactly one trigger associated with it. If you want to use multiple triggers, you must create multiple functions.





110
Azure Functions
Reasons to choose WebJobs over Azure Functions

You have specific customizations that you want to make to the JobHost that aren't supported by Azure Functions.
You want to control your app's retry policies.
WebJobs only supports C# on Microsoft Windows.





111
Azure Functions
Three properties are required in all bindings, though you might have to supply more properties based on the type of binding and storage you're using.

Name - Defines the function parameter through which you access the data. For example, in a queue input binding, this property is the name of the function parameter that receives the queue message content.
Type - Identifies the type of binding. For example, the type of data or service you want to interact with.
Direction - Indicates the direction data is flowing. For example, is it an input or output binding?





112
Azure Functions
2. Suppose your Azure Function has a blob trigger associated with it and you want it to execute only when png images are uploaded. Which of the following blob trigger Path values should you use? 

samples-workitems/{name}.png
The Path tells the blob trigger where it should monitor for changes, and if there are any filters applied. Adding a file extension to the Path specifies that uploaded files must have that file extension in order for the trigger to invoke the function.





113
Azure Functions
Just like the other triggers we've seen so far, you can create a blob trigger in the Azure portal. Inside your Azure function, select Blob trigger from the list of predefined trigger types. Then, enter the logic that you want to execute when a blob is created or updated.

One setting that's important to understand is the Path. The Path tells the blob trigger which blob container to monitor to see if a blob is uploaded or updated. By default, the Path value is:

samples-workitems/{name}





114
Azure Functions
In an blob input binding definition:
      "path": "thumbnail-images/{filename}",
Provide the path, which specifies the container and the item name that goes in it. The path property is required when using the blob trigger, and should be provided in the style shown here, with curly braces around the filename portion of the path. This syntax creates a binding expression that allows you to reference the blob's name in other bindings, and in your function's code.





115
Azure Functions
Bindings comes in two flavors, input, and output. Input bindings can be used to pass data to your function from a data source different than the one that triggered the function.





116
Azure Functions
By default, functions have a timeout of five (5) minutes. This timeout is configurable to a maximum of 10 minutes. If your function requires more than 10 minutes to execute, you can host it on a VM. Additionally, if your service is initiated through an HTTP request and you expect that value as an HTTP response, the timeout is further restricted to 2.5 minutes.





117
Azure Functions
API Management	APIM provides security and routing for your HTTP triggered function endpoints as a way to expose them as a true REST API.





118
Python
Python sequences have three common characteristics:

  They are iterable, which means you can iterate through them.
  They have a length, which means you can pass them to len() to get the number of elements it contains.
  An element of a sequence can be accessed based on its position in the sequence using an integer index.





119
Python
The load_tests() function in Python’s unittest framework is a hook for customizing test loading and suite creation. It takes three arguments: a test loader, a series of tests, and a test discovery pattern. When you call unittest.main(), load_tests() gets called automatically.





120
Python
The response headers can give you useful information, such as the content type of the response payload and a time limit on how long to cache the response. To view these headers, access .headers:

>>> import requests

>>> response = requests.get("https://api.github.com")
>>> response.headers
{'Server': 'GitHub.com',
...
'X-GitHub-Request-Id': 'AE83:3F40:2151C46:438A840:65C38178'}





121
Python
Many popular machine learning algorithms and datasets are built into TensorFlow and are ready to use. In addition to the built-in datasets, you can access Google Research Datasets or use Google’s Dataset Search to find even more.





122
Python
In Python, you can compare sequences of the same type. When comparing two sequences, the first 
non-equal
 value determines the outcome.

For example, consider the following lists:

numbers = [1, 2, 3, 4, 5]
more_numbers = [1, 2, 2, 10, 20]
When you compare these lists, you’ll find that numbers is greater than more_numbers.





123
Python
The CRUD operations correspond almost one-to-one with SQL commands:

Create: INSERT
Read: SELECT
Update: UPDATE
Delete: DELETE
So, to modify existing data in SQL, you would use the UPDATE command.





124
Python
Authentication helps a service understand who you are. Typically, you provide your credentials to a server by passing data through the Authorization header or a custom header defined by the service. All the functions of Requests that you’ve seen to this point provide a parameter called auth, which allows you to pass your credentials:

>>> import requests

>>> response = requests.get(
...     "https://httpbin.org/basic-auth/user/passwd",
...     auth=("user", "passwd")
... )





125
Python
More coding languages are supported in TensorFlow than in PyTorch, which has a C++ API. You can use TensorFlow in both JavaScript and Swift. If you don’t want to write much low-level code, then Keras abstracts away a lot of the details for common use cases so you can build TensorFlow models without sweating the details.





126
Python
In Python, special methods support comparison operators.

For example, the .__lt__() method supports the < operator, .__gt__() supports >, and the .__eq__() method supports the == operator.

These methods allow you to define custom behavior for comparison operations in your custom classes.





127
Python
To make a class iterable, you need to define the .__iter__() special method in the class. This method must return an iterator, which Python uses in its iteration protocol.





128
Python
import re

def replace_punctuations_and_spaces(input_string):
    # Define the pattern to match all punctuations and spaces
    pattern = r'[^\w]'
    # Replace all matched characters with an empty string
    result = re.sub(pattern, '', input_string)
    return result





129
Python
To create an iterator in Python, you need to implement .__iter__() and .__next__().

The .__iter__() special method initializes the iterator and must return an iterator object, typically self.

The .__next__() method retrieves the next value in the data stream and must raise a StopIteration exception when the data stream is exhausted.





130
Python
Sessions are used to persist parameters across requests. For example, if you want to use the same authentication across multiple requests, then you can use a session:

import requests

TOKEN = "<YOUR_GITHUB_PA_TOKEN>"

with requests.Session() as session:
    session.auth = TokenAuth(TOKEN)

    first_response = session.get("https://api.github.com/user")
    second_response = session.get("https://api.github.com/user")





131
Python
If you invoke Response.raise_for_status(), then Requests will raise an HTTPError for status codes between 400 and 600. If the status code indicates a successful request, then the program will proceed without raising that exception.





132
Python
According to the HTTP specification, POST, PUT, and the less common PATCH requests pass their data through the message body rather than through parameters in the query string. Using Requests, you’ll pass the payload to the corresponding function’s data parameter.

data takes a dictionary, a list of tuples, bytes, or a file-like object.





133
Python
Because the decoding of bytes to a str requires an encoding scheme, Requests will try to guess the encoding based on the response’s headers if you don’t specify one. You can provide an explicit encoding by setting .encoding before accessing .text:

>>> response.encoding = "utf-8"  # Optional: Requests infers this.
>>> response.text
'{"current_user_url":"https://api.github.com/user", ...}'





134
Python
When you make a request, the Requests library prepares the request before actually sending it to the destination server. Request preparation includes things like validating headers and serializing JSON content.

You can view the PreparedRequest object by accessing .request on a Response object:





135
Python
The print() function allows you to control the format of the output using keyword arguments like sep and end.

The sep argument determines how to separate multiple objects, while the end argument specifies what to print at the end of the output instead of the default newline.





136
Python
On UNIX-like systems, readline is part of Python’s standard library, so you don’t need to install anything. On modern Windows versions, you don’t even need to work with readline because these advanced editing capabilities are already built into the console natively.





137
Python
In computer programming jargon, a heisenbug is a software bug that seems to disappear or alter its behavior when one attempts to study it.[1] The term is a pun on the name of Werner Heisenberg, the physicist who first asserted the observer effect of quantum mechanics, which states that the act of observing a system inevitably alters its state.





138
Python
You can enhance the input functionality by importing the readline module.

This module provides advanced input capabilities, such as command-line editing and history recall, by overriding the standard input processing and registering handlers for keyboard input.





139
Diffusers Library
Markov Chain
A sequence of random variables where each state depends only on the previous one. In diffusion models, the noise addition process is modeled as a Markov chain, allowing controlled transitions from clear images to noisy ones and vice versa, simplifying the math for training.





140
Diffusers Library
Text-Guided Image-to-Image Translation: Adapt an image guided by a text prompt
    Pipeline: img2img
Text-Guided Image-Inpainting: Fill the masked part of an image given the image, the mask, and a text prompt
    Pipeline: inpaint





141
Diffusers Library
Stochastic Differential Equations (SDEs)
These equations describe how noise evolves over continuous time in diffusion models. By defining the noise dynamics, SDEs allow for modeling both forward (adding noise) and reverse (removing noise) processes as smooth, differential transformations.





142
Diffusers Library
There are three main components of the library to know about:
Many different schedulers - algorithms that control how noise is added for training, and how to generate denoised images during inference.





143
Diffusers Library
Unconditional Image Generation: Generate an image from Gaussian noise
    Pipeline: unconditional_image_generation 
Text-Guided Image Generation: Generate an image given a text prompt
    Pipeline: conditional_image_generation





144
Diffusers Library
ELBO (Evidence Lower Bound)
A loss function that diffusion models optimize to maximize the likelihood of generating realistic data. ELBO breaks down the generative process into manageable steps, penalizing differences between real and generated data distributions, balancing reconstruction quality and computational efficiency.





145
Diffusers Library
Classifier-Free Guidance
A technique allowing diffusion models to generate images without an external classifier, enhancing flexibility in output. This guidance uses the model's own internal structure to fine-tune the output, improving realism without the need for additional label constraints.





146
Diffusers Library
There are three main components of the library to know about:
Popular pretrained model architectures and modules that can be used as building blocks for creating diffusion systems.





147
Diffusers Library
Denoising Process
The step-by-step refinement in diffusion models where the model removes noise from a random input, progressing towards an image. At each step, the model estimates the noise in the image and subtracts it, gradually enhancing coherence and detail.





148
Diffusers Library
Forward and Reverse Processes
In diffusion, the forward process gradually adds noise to data, following a fixed probabilistic schedule, and the reverse process learns to denoise the noisy data. Mathematically, the forward process models q(x_t | x_{t-1}), while the reverse learns p(x_{t-1} | x_t) to reconstruct images.





149
Diffusers Library
Variational Inference
A technique for approximating complex probability distributions by simpler ones. In diffusion models, variational inference approximates the posterior distribution of the reverse process, enabling computationally feasible training by simplifying the diffusion and denoising steps.





150
Diffusers Library
Reverse Diffusion
The inverse process where an image is generated by removing noise, step-by-step, from an initial noisy state. This approach is essential in diffusion models, reconstructing data by "reversing" the diffusion process and achieving high-quality image outputs.





151
Diffusers Library
Score Matching
A method to train models by estimating the gradient of data probability density (the "score") at each noise level. By approximating this gradient, the model learns to shift noisy data towards regions of higher probability, ultimately leading to denoised, realistic images.





152
Diffusers Library
Running pip install within a Jupyter notebook is straightforward: you prefix the command with an exclamation mark (!). This tells the notebook to execute the command as a shell command rather than Python code. For example:

python
!pip install ipywidgets

After installing, remember to restart the kernel to ensure the notebook recognizes the new packages.





153
Diffusers Library
Likelihood Estimation
The process of calculating the probability of the data under the model's parameters. In diffusion models, maximizing this likelihood means finding parameters that make the observed data most probable, allowing for a better approximation of complex data distributions.





154
Diffusers Library
Text-Guided Depth-to-Image Translation: adapt parts of an image guided by a text prompt while preserving structure via depth estimation
    pipeline: depth2img

Start by creating an instance of a DiffusionPipeline and specify which pipeline checkpoint you would like to download.





155
Home Network Project
Front: What is Docker Compose used for in managing containers?

Back: Docker Compose is a tool that allows users to define and manage multi-container Docker applications using a YAML file (docker-compose.yml). It simplifies the orchestration of multiple services, enabling easy deployment, scaling, and configuration of interconnected containers with a single command.





156
Home Network Project
Front: What is the purpose of Volumes in Docker?

Back: Volumes in Docker are used to persist data generated by and used by Docker containers. They allow data to survive container restarts and enable data sharing between containers. Volumes are managed by Docker and can be easily backed up, restored, and shared across different containers and environments.





157
Home Network Project
Front: How does WireGuard handle encryption?

Back: WireGuard uses modern cryptographic algorithms like Curve25519 for key exchange, ChaCha20 for encryption, Poly1305 for data authentication, and BLAKE2s for hashing. These algorithms provide high security while maintaining performance and efficiency.





158
Home Network Project
Front: What are the key components required to set up a WireGuard VPN Server?

Back: The key components include:

WireGuard Software: Installed on the server (e.g., Raspberry Pi).
Public and Private Keys: For encryption and authentication.
Configuration Files: Define interface settings, peers, allowed IPs, and routing rules.
Network Configuration: Proper port forwarding on the router and firewall rules to allow VPN traffic.





159
Home Network Project
Front: What command is used to generate a new SSH key pair?

bash
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"





160
Home Network Project
Front: What is Prometheus and what is its role in network monitoring?

Back: Prometheus is an open-source monitoring and alerting toolkit designed for reliability and scalability. It collects and stores metrics from various services and systems, allowing users to query and analyze performance data to monitor the health and performance of their network and applications.





161
Home Network Project
Front: Define SSL/TLS Handshake.

Back: The SSL/TLS Handshake is the process by which a client and server establish a secure connection. It involves exchanging cryptographic keys, authenticating the server (and optionally the client), and negotiating encryption algorithms to ensure secure communication.





162
Home Network Project
Front: Describe the primary function of Grafana in a monitoring stack.

Back: Grafana is an open-source platform for data visualization and analytics. It connects to various data sources like Prometheus, enabling users to create interactive and customizable dashboards to visualize metrics, monitor system performance, and gain insights into network and application behavior.





163
Home Network Project
Front: What is Snort and how is it used in network security?

Back: Snort is an open-source network intrusion detection and prevention system (IDS/IPS). It analyzes network traffic in real-time, using predefined rules to detect malicious activities such as exploits, port scans, and unauthorized access attempts. Snort can be integrated into pfSense to enhance network security by identifying and responding to potential threats.





164
Home Network Project
How do you install Docker on a Raspberry Pi?

bash
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker pi





165
Home Network Project
Front: What is SSL Certificate and why is it important for web services?

Back: An SSL Certificate is a digital certificate that authenticates the identity of a website and enables encrypted connections via SSL/TLS protocols. It ensures secure data transmission between the user's browser and the web server, protecting sensitive information from interception and tampering.





166
Home Network Project
Front: Explain the difference between a Docker Image and a Docker Container.

Back:
Docker Image: A read-only template that contains the application and all its dependencies required to run it. Images are built from Dockerfiles and serve as the blueprint for containers.
Docker Container: A runnable instance of a Docker image. Containers are created from images and can be started, stopped, moved, and deleted independently.





167
Home Network Project
Front: What is Network Latency and how can it affect network performance?

Back: Network Latency refers to the time it takes for data to travel from the source to the destination across a network. High latency can result in delays, slow application performance, lag in real-time communications, and poor user experiences, especially in activities like gaming, video conferencing, and streaming.





168
Home Network Project
Front: What is Service Mesh and how does it enhance microservices architecture?

Back: A Service Mesh is a dedicated infrastructure layer that manages service-to-service communication within a microservices architecture. It provides features like load balancing, service discovery, traffic management, security (encryption and authentication), and observability (monitoring and tracing), enhancing the reliability and efficiency of microservices interactions.





169
Home Network Project
Front: What are Service Discovery and its importance in containerized environments?

Back: Service Discovery is the process of automatically detecting and locating services within a network, particularly in dynamic environments like container orchestration platforms. It enables containers to find and communicate with each other without manual configuration, facilitating scalability and efficient resource management.





170
Home Network Project
Front: What is WireGuard and how does it differ from traditional VPN protocols?

Back: WireGuard is a modern, open-source VPN protocol known for its simplicity, high performance, and strong security. Unlike traditional VPN protocols like OpenVPN or IPSec, WireGuard has a smaller codebase, making it easier to audit and less prone to vulnerabilities. It uses state-of-the-art cryptography to provide secure and efficient encrypted connections.





171
Home Network Project
Front: What is pfSense and what are its primary uses?

Back: pfSense is an open-source firewall and router software based on FreeBSD. It provides advanced networking features such as traffic shaping, VPN support, intrusion detection and prevention, load balancing, and more. It's commonly used for securing and managing both home and enterprise networks.





172
Home Network Project
Front: What is QoS (Quality of Service) in networking?

Back: QoS refers to the set of technologies and techniques used to manage network resources by prioritizing certain types of traffic over others. It ensures that critical applications (like VoIP or streaming) receive the necessary bandwidth and low latency, improving overall network performance and user experience.





173
Home Network Project
Front: Define Reverse Proxy and its common uses.

Back: A Reverse Proxy is a server that sits between client devices and backend servers, forwarding client requests to the appropriate server. Common uses include load balancing, caching, SSL termination, and enhancing security by hiding the identities and structures of backend servers.





174
Home Network Project
Front: What is a Peer in WireGuard terminology?

Back: In WireGuard, a peer is an endpoint in the VPN network. Each peer has its own public and private keys and a configuration that specifies allowed IPs and connection details. Peers communicate directly with each other once the VPN is established, forming a secure mesh network.





175
Home Network Project
Front: What is Port Forwarding and how is it configured on a router?

Back: Port Forwarding is the process of directing incoming traffic on specific ports from the router to designated devices within the local network. It is configured through the router’s admin interface by specifying the external port, internal IP address, and internal port, enabling services like VPNs, game servers, or web servers to be accessible externally.





176
Home Network Project
Front: What is Nextcloud and how is it utilized in a home network?

Back: Nextcloud is an open-source, self-hosted cloud storage and collaboration platform. In a home network, it can be used to store, share, and synchronize files, manage calendars and contacts, and collaborate on documents. It provides functionality similar to services like Dropbox or Google Drive but with full control over data privacy and security.





177
Home Network Project
Front: Explain the concept of Port Forwarding and its use in VPN setup.

Back: Port Forwarding is the process of redirecting communication requests from one address and port number combination to another. In VPN setups, it allows external VPN clients to connect to the VPN server by forwarding the necessary ports (e.g., UDP 51820 for WireGuard) from the router to the internal VPN server's IP address, enabling secure remote access to the home network.





178
Home Network Project
Front: What is Docker Swarm and how does it differ from Kubernetes?

Back: Docker Swarm is Docker’s native clustering and orchestration tool for managing a group of Docker engines (nodes) as a single virtual system. It simplifies deploying and managing containerized applications across multiple hosts. Kubernetes, on the other hand, is a more feature-rich and widely adopted container orchestration platform developed by Google, offering advanced features like automated scaling, rolling updates, and self-healing, but with a steeper learning curve.





179
Home Network Project
Front: What is Port Mapping in Docker and why is it important?

Back: Port Mapping in Docker connects a port on the host machine to a port inside a Docker container. It allows external access to services running within containers. For example, mapping host port 80 to container port 80 enables users to access a web server running inside the container via the host's IP address on port 80.





180
Home Network Project
Front: What is Rate Limiting and how does it help in preventing attacks?

Back: Rate Limiting restricts the number of requests a user or IP address can make to a service within a specific time frame. It helps prevent various attacks, such as brute-force attacks, denial-of-service (DoS) attacks, and abuse of APIs, by limiting the potential impact of malicious activities.





181
Home Network Project
Front: How does Docker Networking facilitate communication between containers?

Back: Docker Networking allows containers to communicate with each other and with external networks through virtual networks. Docker provides several network drivers (e.g., bridge, host, overlay) that define how containers connect and interact. Custom networks can be created to isolate groups of containers or enable specific communication patterns.





182
Home Network Project
What is the command to update and upgrade packages on a Debian-based system like Raspberry Pi OS?

bash
sudo apt update && sudo apt upgrade -y





183
Home Network Project
Front: What is a Docker Compose file and what is it used for?

Back: A Docker Compose file (docker-compose.yml) is a YAML file that defines and configures multi-container Docker applications. It specifies the services, networks, and volumes needed for the application, allowing users to start and manage all containers with a single command (docker-compose up).





184
Home Network Project
Front: What is a Prometheus Exporter?

Back: A Prometheus Exporter is an application or service that collects and exposes metrics from a system, application, or service in a format that Prometheus can scrape and store. Examples include node_exporter for system metrics and cAdvisor for container metrics.





185
Home Network Project
Front: What command starts a Docker container for Prometheus using the official image?

bash
docker run -d --name=prometheus -p 9090:9090 prom/prometheus





186
Home Network Project
Front: What is Fail2Ban and how does it protect a network?

Back: Fail2Ban is an intrusion prevention software that monitors log files for suspicious activity, such as repeated failed login attempts. When such patterns are detected, Fail2Ban automatically updates firewall rules to temporarily or permanently ban offending IP addresses, protecting the network from brute-force attacks and unauthorized access.





187
Home Network Project
Front: How does Pi-hole utilize DNS Sinkholing to block ads?

Back: Pi-hole intercepts DNS queries for domains listed in its blocklists. Instead of resolving these domains to their legitimate IP addresses, Pi-hole returns a non-routable IP address (e.g., 0.0.0.0), preventing the ad content from loading on the client devices.





188
Home Network Project
Front: How do Firewall Rules work in pfSense?

Back: Firewall rules in pfSense define how incoming and outgoing network traffic is handled. They are based on criteria like source/destination IP addresses, ports, and protocols. Rules can allow, block, or reject traffic, and are processed in a specific order to control access and protect the network from unauthorized activities.





189
Home Network Project
Front: Explain the purpose of AllowedIPs in WireGuard configuration.

Back: AllowedIPs defines the IP addresses that are permitted to be routed through the VPN tunnel for a specific peer. It acts as both an access control list and routing rule, determining which traffic should be encrypted and sent through the VPN and which should bypass it.





190
Home Network Project
Front: How can you whitelist a domain in Pi-hole, and why would you do it?

Back: To whitelist a domain in Pi-hole, you add the domain to the whitelist through the Pi-hole admin interface or using command-line tools. Whitelisting ensures that Pi-hole allows DNS resolution for that domain, which is useful for preventing the blocking of legitimate services or applications that rely on specific domains.





191
Home Network Project
Front: What is Bandwidth and how does it differ from Throughput?

Back:

Bandwidth: The maximum rate of data transfer across a network path, typically measured in Mbps or Gbps. It represents the network's capacity.
Throughput: The actual rate of successful data transfer achieved, often lower than the bandwidth due to factors like network congestion and protocol overhead.





192
Home Network Project
Front: How can you view the current DNS settings on a Linux system?

bash
cat /etc/resolv.conf





193
Home Network Project
Front: How do Dashboards in Grafana enhance network monitoring?

Back: Dashboards in Grafana provide a visual representation of collected metrics through graphs, charts, and tables. They allow users to monitor real-time and historical data, identify trends, detect anomalies, and make informed decisions based on comprehensive visual analytics.





194
Home Network Project
Front: What is DNS over HTTPS (DoH) and its advantages?

Back: DNS over HTTPS (DoH) is a protocol that encrypts DNS queries by sending them over HTTPS. Advantages include:

Privacy: Prevents eavesdropping and manipulation of DNS traffic.
Security: Protects against DNS spoofing and man-in-the-middle attacks.
Censorship Resistance: Makes it harder for third parties to block or redirect DNS queries.





195
Home Network Project
Front: What is TLS Termination and why is it used in reverse proxies?

Back: TLS Termination refers to the process of decrypting incoming TLS/SSL encrypted traffic at the reverse proxy level before forwarding it to backend servers. It offloads the computationally intensive decryption task from backend servers, improves performance, and allows for centralized management of SSL certificates.





196
Home Network Project
Front: Explain PromQL in the context of Prometheus.

Back: PromQL (Prometheus Query Language) is a powerful query language used to retrieve and manipulate time-series data stored in Prometheus. It allows users to perform calculations, aggregations, and transformations to analyze metrics and generate meaningful insights.





197
Home Network Project
Front: What is the purpose of the docker-compose.yml file?

Back: The docker-compose.yml file defines and configures multi-container Docker applications. It specifies the services, networks, and volumes required for the application, enabling users to deploy and manage all containers with a single command (docker-compose up).





198
Search Engine Optimization
SEO Spider by Screaming Frog is a website crawling tool that helps identify SEO issues such as broken links, duplicate content, and missing metadata. It assists in conducting comprehensive site audits to improve a website’s SEO performance and search engine rankings.





199
Search Engine Optimization
A website crawler is a tool that systematically browses the web to index and analyze website content. It identifies SEO issues, broken links, and content structure, providing valuable insights for optimization and ensuring that websites are efficiently indexed by search engines.





200
Search Engine Optimization
Answer The Public is a keyword research tool that visualizes search questions and autocomplete suggestions. It helps users generate content ideas by uncovering what questions and topics people are searching for related to specific keywords.





201
Search Engine Optimization
An XML sitemap is a file that lists all the important pages of a website, helping search engines crawl the site more effectively. It improves the discoverability of webpages, ensures all content is indexed, and can include additional metadata about each URL.





202
Search Engine Optimization
The Mobile-Friendly Test is a tool by Google that assesses how easily a visitor can use a page on a mobile device. It ensures websites are optimized for mobile users, which is a significant ranking factor, as search engines prioritize mobile-friendly sites in their results.





203
Search Engine Optimization
Yoast SEO is a WordPress plugin that assists in optimizing website content for better SEO. It provides tools for managing meta tags, generating sitemaps, and improving content readability, helping users enhance their site’s search engine visibility and performance.





204
Search Engine Optimization
A backlink is an incoming hyperlink from one webpage to another website. Backlinks are crucial for SEO as they signal authority and trustworthiness to search engines, thereby improving a website’s search engine rankings and visibility.





205
Search Engine Optimization
Bing Webmaster Tools is a free service by Bing that assists webmasters in managing their site’s presence in Bing search results. It offers tools for site scanning, keyword research, backlink analysis, and performance reporting to optimize SEO.





206
Search Engine Optimization
Keyword density is the percentage of times a keyword appears on a webpage compared to the total number of words. It is used to optimize content for relevant keywords without overstuffing, ensuring that the content remains natural and readable while signaling relevance to search engines.





207
Search Engine Optimization
Crawl errors are issues that prevent search engine bots from accessing or indexing certain pages on a website. Identifying and fixing these errors improves website visibility in search results by ensuring that all important pages are accessible to search engines.





208
Search Engine Optimization
Duplicate content refers to identical or very similar content appearing on multiple webpages or across different websites. It can negatively impact SEO by causing confusion for search engines about which page to rank, potentially diluting search rankings.





209
Search Engine Optimization
On-page SEO involves optimizing individual web pages to rank higher and earn more relevant traffic in search engines. This includes optimizing content, HTML elements, and internal links to improve the page’s visibility and relevance for targeted keywords.





210
Search Engine Optimization
SEOquake is a browser extension that provides on-page SEO audits, keyword analysis, and competitive insights directly within the browser. It facilitates quick SEO evaluations without leaving the webpage, offering valuable data for optimizing content and strategies.





211
Search Engine Optimization
Content quality refers to the overall value and relevance of the information provided on a website. High-quality content engages users, addresses their needs, and is essential for improving SEO rankings by demonstrating authority and expertise to search engines.





212
Search Engine Optimization
Link Explorer by Moz is a tool for analyzing a website’s backlink profile and domain authority. It helps users identify linking opportunities, monitor backlink quality, and understand the overall strength of their site’s SEO through detailed backlink data.





213
Search Engine Optimization
Google PageSpeed Insights analyzes the content of a web page and provides suggestions to make it faster. It evaluates both desktop and mobile performance, offering recommendations to improve load times and overall website performance for better user experience.





214
Search Engine Optimization
Ahrefs is a comprehensive SEO toolset used for backlink analysis, keyword research, competitor analysis, and site audits. It helps users identify backlink opportunities, monitor website performance, and gain insights into competitors' strategies to enhance their own SEO efforts.





215
Search Engine Optimization
Robots.txt is a text file placed in the root directory of a website that instructs search engine crawlers on which pages or sections to crawl or avoid. It helps manage crawler access, preventing the indexing of certain parts of a website to control search engine visibility.





216
Search Engine Optimization
Domain Authority (DA), developed by Moz, is a metric that predicts how well a website will rank on search engine result pages (SERPs). It is based on various factors, including the quality and quantity of backlinks, and helps gauge a site’s overall SEO strength.





217
Search Engine Optimization
Crawling is the process by which search engine bots systematically browse the web to discover and index content. It is essential for search engines to include webpages in their search results, ensuring that new and updated content is recognized.





218
Search Engine Optimization
Link building is the practice of acquiring backlinks from other websites to improve a site's authority and search rankings. It is critical for enhancing a website’s SEO performance by increasing the number and quality of inbound links from reputable sources.





219
Search Engine Optimization
Google Keyword Planner is a free tool within Google Ads that assists in discovering keywords and providing search volume data. It is used for keyword research and planning SEO strategies by identifying relevant and high-performing keywords to target.





220
Search Engine Optimization
A canonical tag is an HTML element that specifies the preferred version of a webpage to prevent duplicate content issues. By indicating the canonical URL, it helps consolidate ranking signals and ensures search engines index the correct page.





221
Search Engine Optimization
Link equity refers to the value or authority passed from one page to another through hyperlinks. It influences the ranking potential of linked pages, helping them to perform better in search engine results by distributing authority throughout the website.





222
Search Engine Optimization
Google Analytics is a free web analytics service that tracks and reports website traffic. It provides insights into user behavior, traffic sources, and conversion metrics, enabling users to understand how visitors interact with their site and measure the effectiveness of their marketing efforts.





223
Power BI
Creating POST Requests in Power Query: You can modify Power Query M code to create a POST request by specifying the endpoint URL, headers (such as Content-Type: application/json), and request body. The body is structured as JSON using Json.FromValue(), converted to binary with Text.ToBinary(), and passed to the Web.Contents() function.





224
Power BI
A warning with query diagnostics, as mentioned elsewhere, is that you'll see drastically different capabilities depending on the connector. For example, many ODBC based connectors won't have an accurate recording of what query is send to the actual backend system, as Power Query only sees what it sends to the ODBC driver.





225
Power BI
The Usage Metrics Report semantic model contains usage data for the last 30 days. It can take up to 24 hours for new usage data to be imported. You can't trigger a manual refresh by using the Power BI user interface.





226
Power BI
The improved usage metrics report includes the following report pages:

Report usage Provides information about report views and report viewers, such as how many users viewed the report by date.
Report performance Shows the typical report opening times broken down by consumption method and browser types.
FAQ Provides answers to frequently asked questions, such as What is a "Viewer" and what is a "View"?





227
Power BI
If your organization is using Azure Private Link in Power BI, because client-telemetry is not available the usage metrics reports will only contain Report Open events.





228
Power BI
Power BI and API Calls: Power BI supports REST API calls, allowing data import from web services. The built-in Web connector is used for API integration but primarily supports GET requests. More complex methods like POST require manual M code configuration to handle the request body and headers.





229
Power BI
Certain types of views aren't included in performance measurements. For example, when a user selects a link to a report in an email message, the Report View is accounted for in the report usage but there is no event in the performance metrics.





230
Power BI
on 9/30, when I was trying to upload a report for HEC, I was noticing that Power BI seemed to go unresponsive and not publish. It turned out that I could publish if I switched from the corp network to the guest network. Why this works is a mystery.





231
Power BI
Handling Optional API Parameters: For more advanced API requests, parameters such as catalog, calculations, annual averages, and aspects can be added in the payload. These allow for more control over the data returned. Adding these parameters will depend on the API’s available options as per its documentation.





232
Power BI
The SUMMARIZE function in DAX is a powerful function used to create summary tables or groupings of data based on specified columns and expressions. It essentially performs a GROUP BY operation similar to SQL.





233
Power BI
Power BI Web Connector Limitation: The "From Web" connector supports only GET requests by default. To use POST requests or include a body/payload in the request, you must manually edit the Power Query M code. The connector does not provide an interface for specifying request types or payloads in the UI.





234
MS Identity Platform
OpenID Connect versus SAML: The platform uses both OpenID Connect and SAML to authenticate a user and enable single sign-on. SAML authentication is commonly used with identity providers such as Active Directory Federation Services (AD FS) federated to Microsoft Entra ID, so it's often used in enterprise applications.





235
MS Identity Platform
URIs for certain apps:
○ Apps that use embedded browsers: https://login.microsoft.com/common/oauth2/nativeclient (Note: if your app would pop up a window which typically contains no address bar, it is using the "embedded browser")
Apps that would use system browsers: http://loclahost (Note: If your app would bring your system's default browser (such as edge, chrome, firefox, and so on) to visit Microsoft login portal, it is using the "system browser".)





236
MS Identity Platform
If your desktop or mobile application runs on Windows and on a machine connected to a Windows domain (Active Directory or Microsoft Entra joined) it is possible to use the Integrated Windows Authentication (IWA) to acquire a token silently. No UI is required when using the application.





237
MS Identity Platform
ID token - ID tokens are sent to the client application as part of an OpenID Connect flow. They can be sent alongside or instead of an access token. ID tokens are used by the client to authenticate the user. To learn more about how the Microsoft identity platform issues ID tokens, see ID tokens in the Microsoft identity platform.





238
MS Identity Platform
Client - The client in an OAuth exchange is the application requesting access to a protected resource. The client could be a web app running on a server, a single-page web app running in a user's web browser, or a web API that calls another web API. You'll often see the client referred to as client application, application, or app.





239
MS Identity Platform
The parties in an authentication flow use bearer tokens to assure, verify, and authenticate a principal (user, host, or service) and to grant or deny access to protected resources (authorization). Bearer tokens in the Microsoft identity platform are formatted as JSON Web Tokens (JWT).





240
MS Identity Platform
Authorization server - The Microsoft identity platform is the authorization server. Also called an identity provider or IdP, it securely handles the end-user's information, their access, and the trust relationships between the parties in the auth flow. The authorization server issues the security tokens your apps and APIs use for granting, denying, or revoking access to resources (authorization) after the user has signed in (authenticated).





241
MS Identity Platform
OAuth versus OpenID Connect: The platform uses OAuth for authorization and OpenID Connect (OIDC) for authentication. OpenID Connect is built on top of OAuth 2.0, so the terminology and flow are similar between the two. You can even both authenticate a user (through OpenID Connect) and get authorization to access a protected resource that the user owns (through OAuth 2.0) in one request.





242
MS Identity Platform
Resource owner - The resource owner in an auth flow is usually the application user, or end-user in OAuth terminology. The end-user "owns" the protected resource (their data) which your app accesses on their behalf. The resources owners can grant or deny your app (the client) access to the resources they own. For example, your app might call an external system's API to get a user's email address from their profile on that system.





243
MS Identity Platform
Confidential client applications: Apps in this category include:

Web apps that call a web API
Web APIs that call a web API
Daemon apps, even when implemented as a console service like a Linux daemon or a Windows service





244
MS Identity Platform
Authentication scenarios involve two activities:

Acquiring security tokens for a protected web API: We recommend that you use the Microsoft Authentication Library (MSAL), developed and supported by Microsoft.
Protecting a web API or a web app: One challenge of protecting these resources is validating the security token. On some platforms, Microsoft offers middleware libraries.





245
PP365
Syntax for using the web api to get specific staff records based on a substring in the name column

FActs_endpoint = "ts_staffs?$filter=contains(ts_name, 'Manu')"
staff_records = make_api_request(FActs_endpoint, method='Get')

ts_staffs: table name (pluralized)
filter=contains(ts_name, 'Manu'): filter on specific field based on containing a substring





246
PP365
You can set a sandbox, production, or trial (subscription-based) environment in administration mode so that only users with System Administrator or System Customizer security roles will be able to sign in to that environment. Administration mode is useful when you want to make operational changes and not have regular users affect your work, and not have your work affect end users (non-admins).





247
PP365
Workflow in the context of Microsoft Dataverse and Power Apps refers to the automation of business processes and tasks. Workflows are a series of steps and rules that define how data is processed and managed within the system. Workflows can be triggered by events such as record creation, update, or deletion, and can perform actions like sending emails, updating records, creating tasks, and more.





248
PP365
Background operations (optional)	Select to disable all asynchronous operations (see Asynchronous service) such as workflows and synchronization with Exchange. Emails will not be sent and server-side synchronization for appointments, contacts, and tasks are disabled. Note: Administration mode must be enabled to disable background operations.





249
PP365
To support offline use, Power Apps allows data to be cached locally on the device. Users can create, update, and delete records while offline, and these changes are synchronized with the Dataverse database once the device reconnects to the internet. This must all be configured ahead of time.





250
PP365
You can call on-demand workflows from inside a business process flow. You can configure this from the new business process flow designer by dragging a workflow component to a process stage or to the Global Workflows section.





251
PP365
In most cases, adding users to an environment only gives users access to the environment itself, not to any resources (apps and data) in the environment. You need to configure access to resources by assigning security roles to users.





252
PP365
To reassign (ownership) of all records for a user, the option is in the admin portal > settings > user > select user > somewhere on that blade.





253
PP365
Plugin Registration Tool
A utility provided by Microsoft for registering plugins, custom workflows, and webhooks in Dynamics 365. It allows developers to deploy and manage custom server-side code that executes in response to events within Dynamics 365, facilitating integration and customization.





254
PP365
Ribbon
The toolbar interface in Dynamics 365 where users interact with commands and actions. Customizing the ribbon allows administrators to add, remove, or modify buttons and actions, enhancing the user experience and enabling integration with external services like Azure Functions without altering the core application code.





255
PP365
Managed Identities
A feature in Azure that provides Azure services with an automatically managed identity in Azure Active Directory (Azure AD). Managed identities simplify authentication to other Azure services by eliminating the need to manage credentials manually, enhancing security and ease of use.





256
PP365
API Management (APIM)
A fully managed Azure service that enables organizations to publish, secure, transform, maintain, and monitor APIs. APIM provides features like rate limiting, API gateways, developer portals, and analytics, helping manage the full lifecycle of APIs and ensuring secure and scalable access to backend services.





257
PP365
System-Assigned Managed Identity
A type of managed identity in Azure that is directly tied to an individual Azure resource, such as an Azure Function. The lifecycle of a system-assigned managed identity is bound to the resource; it is automatically created when enabled and deleted when the resource is removed.





258
PP365
Azure Managed Identity
Another term for Managed Identities in Azure. It refers to the feature that provides Azure services with an automatically managed identity in Azure AD, facilitating secure authentication to other Azure resources without the need to handle credentials manually.





259
PP365
User-Assigned Managed Identity
A managed identity in Azure that exists as a separate Azure resource and can be assigned to multiple Azure services. Unlike system-assigned identities, user-assigned identities are independent of any single resource’s lifecycle, offering flexibility and reusability across different services.





260
PP365
I had a ticket with Lauren Kawamoto going back to 11/22/24 about her permissions in the sandbox. It turns out that she needed the "TS - Recur Engagement" role to recur engagements. I thought that "system administrator" would be enough, but I was not able to do that myself with the sysadmin role. Strange.





261
PP365
Webhooks
HTTP callbacks triggered by specific events in applications like Dynamics 365. When an event occurs, the application sends an HTTP request to a predefined URL (e.g., an Azure Function), allowing external systems to respond in real-time without constant polling.





262
PP365
CORS (Cross-Origin Resource Sharing)
A security feature implemented in web browsers that restricts web pages from making requests to a different domain than the one that served the web page. Configuring CORS in Azure Functions allows specific domains (e.g., Dynamics 365) to interact with the function securely.





263
Kali Linux
Kali Linux is maintained by Offsec, the company responsible for the OSCP (Offsec Security Certified Professional) certification exam. Its first version 1.0.0 "moto" was released in March 2013.





264
Kali Linux
As of the latest releases, Kali Linux uses XFCE as its default desktop environment. XFCE is chosen for its lightweight nature and speed, making it ideal for performance-sensitive tasks common in cybersecurity work.





265
Kali Linux
The rockyou.txt file is a renowned wordlist frequently used as a default for various password-cracking tools. It is conveniently located in the /usr/share/wordlists/ directory on Kali.

It's a compressed archive containing over 14 million plaintext passwords exposed in the infamous 2009 data breach at the social platform RockYou. This massive password list remains highly valuable for penetration testing efforts despite being over a decade old.





266
Kali Linux
Hydra is a popular password-cracking tool in Kali Linux, designed for conducting rapid dictionary attacks against various protocols to discover weak passwords. Another notable tool for password cracking in Kali is John the Ripper, which focuses on cracking hashed passwords.





267
Kali Linux
The apt-get update command updates the local package index with the latest information about available packages and their versions from the repositories. In contrast, apt-get upgrade upgrades all the installed packages to their latest versions based on the updated package list.





268
Kali Linux
The chmod (change mode) command is used to change the access permissions of files and directories. Permissions determine who can read, write, or execute a file. Changing the owner of a file is done using the chown command, not chmod.





269
Kali Linux
The /etc/shadow file stores secure user account information, including encrypted passwords. This file is readable only by the root user for security reasons. In contrast, /etc/passwd contains user account details like usernames and user IDs but does not store encrypted passwords.





270
Kali Linux
The first step is to visit the official Kali Linux website and navigate to the download page. You can choose the platform to install it on, such as virtual machines or a bootable USB drive.

Kali allows you to install it in many different environments, including in the cloud, on arm-based devices such as Raspberry Pi, and even through Windows Subsystem for Linux (WSL).





271
Kali Linux
We’ll show you how to install the open-source code editor code-oss, which was built from Microsoft Visual Studio Code.

Head to the terminal and enter:

sudo apt install code-oss





272
Kali Linux
apt (Advanced Package Tool) is the package manager used in Debian-based distributions, including Kali Linux. It simplifies the process of installing, updating, and removing software packages.





273
Diffusers from Hugging Face
Text Encoder (CLIPTextModel): Also from transformers, the text encoder turns tokenized text into embeddings that represent the semantic meaning of the prompt, guiding image generation to reflect the prompt.





274
Diffusers from Hugging Face
Guidance Scale: A float controlling adherence to the prompt, with higher values enforcing a stronger alignment to the prompt, while lower values allow more creativity in the output.





275
Diffusers from Hugging Face
Device Management: In torch, models and data are moved to the GPU (CUDA) with .to(torch_device), enabling faster processing by taking advantage of parallel GPU computations.





276
Diffusers from Hugging Face
AutoencoderKL (VAE): A part of diffusers, this component compresses images into a latent space for efficient computation and then decodes the generated latent representations back into image space.





277
Diffusers from Hugging Face
Prompt Embedding and Conditioning: transformers’ CLIPTokenizer and CLIPTextModel embed both prompt and “unconditioned” input (empty string), allowing flexible control over how strongly the model adheres to the prompt.





278
Diffusers from Hugging Face
Latent Space and Latent Noise: The initial noise, generated with torch.randn, creates latents that represent the starting point in the latent space. This noise is progressively denoised to form the final image.





279
Diffusers from Hugging Face
Tokenizer (CLIPTokenizer): Provided by transformers, it converts input text into token IDs. These numerical representations allow the model to process the prompt and understand its meaning.





280
Diffusers from Hugging Face
Batch Size: In torch, batch size (len(prompt)) controls the number of prompts processed together. For single-prompt processing, it’s set to 1, impacting memory and processing speed.





281
Diffusers from Hugging Face
Scheduler (UniPCMultistepScheduler): A diffusers module that controls the denoising steps of the diffusion process, determining how the image progressively refines with each step toward the prompt.





282
Diffusers from Hugging Face
UNet (UNet2DConditionModel): In diffusers, UNet iteratively removes noise from the latent representation, gradually shaping random noise into an image aligned with the text prompt.





283
OData
$expand
The $expand query option is used to include related entities in the response. For example, if you are querying a customer entity, you can use $expand to retrieve their associated orders or addresses in the same query, enabling easier navigation of related data.





284
OData
Metadata Document
In OData, the metadata document describes the structure of the OData service, including the entity types, relationships, and properties. It is expressed in an XML format and allows developers and tools to understand the underlying schema of the OData service for integration purposes.





285
OData
A Many-to-Many relationship allows multiple records in one entity to be associated with multiple records in another entity. In Dataverse, this is represented using an explicit intersect (junction) entity that holds references to related records from both entities.





286
OData
Metadata in OData describes the structure, data types, relationships, and other aspects of the data model. The $metadata endpoint provides the service’s data model and schema definitions using CSDL, essential for understanding available entities and their relationships.





287
OData
Serialization is the process of converting objects or data structures into a format like JSON or XML for transmission or storage. In OData, serialization ensures that data sent over the Web API is standardized, enabling clients to understand and process it effectively.





288
OData
Inheritance Hierarchies allow entities to derive properties and behaviors from parent entities, enabling reuse and polymorphism. Managed in Dataverse using OData's @odata.type, they allow clients to interact with a hierarchy of related entity types effectively.





289
OData
A Polymorphic Association is a relationship where an entity can be associated with multiple types of other entities. It requires the use of @odata.type to identify the specific type of related entities, enabling flexible and dynamic associations within the data model.





290
OData
Navigation Links are URLs that provide access to related entities or collections within an OData service. They facilitate navigation between related entities, enabling clients to traverse relationships and retrieve associated data seamlessly.





291
OData
Type Casting converts an entity from one type to another within inheritance hierarchies. In OData, it is facilitated by the @odata.type annotation, which specifies the target type for the cast, ensuring accurate data interpretation and processing.





292
OData
An ETag is a mechanism for concurrency control in OData, representing a version identifier for a resource. It ensures that updates or deletions are performed on the correct version of a record, preventing conflicts and maintaining data integrity.





293
OData
Delta Queries allow clients to track changes (additions, deletions, updates) and synchronize data efficiently without retrieving the entire dataset each time. This feature optimizes performance and reduces bandwidth usage by enabling incremental data synchronization.





294
OData
Formatting Instructions are guidelines that specify how data should be presented or displayed. Defined using annotations, they help client applications render data appropriately, such as formatting dates, numbers, or currencies in a user-friendly manner.





295
OData
Annotations provide additional metadata about entities, properties, or relationships in the OData model. They enrich the data with information like display names, validation rules, and formatting instructions, enhancing both data semantics and client-side interactions.





296
OData
A Delta Token is a marker used in Delta Queries to track changes since the last query. Clients use delta tokens to request only the changes (additions, deletions, updates) that have occurred since their last synchronization, optimizing data retrieval.





297
OData
Complex Types are structured data types in OData that group multiple related properties into a single reusable structure. They allow for more organized and modular data models by encapsulating related data, such as addresses or contact information.





298
OData
While not directly related to OData or Dataverse, yield is a programming concept used in iterators to return values one at a time. It is useful in broader API interactions and data processing but is not a primary term in the OData/Dataverse context.





299
OData
The @odata.type annotation specifies the exact entity type or derived type being referenced in an OData payload. It is crucial for handling inheritance and polymorphism by informing clients about the specific type of an entity for correct deserialization and processing.





300
Conditional Access
Find more information about the problem by clicking More Details in the initial error page. Clicking More Details will reveal troubleshooting information that's helpful when searching the Microsoft Entra sign-in events for the specific failure event the user saw or when opening a support incident with Microsoft.
Stuff like App name, App id, IP address, Device identifier, Device Platform,





301
Conditional Access
During sign-in, policies in report-only mode are evaluated but not enforced.
Results are logged in the Conditional Access and Report-only tabs of the Sign-in log details.
Customers with an Azure Monitor subscription can monitor the impact of their Conditional Access policies using the Conditional Access insights workbook.





302
Conditional Access
Report-only: Not applied	
Not all configured policy conditions were satisfied. For example, the user is excluded from the policy or the policy only applies to certain trusted named locations.





303
Conditional Access
Token Renewal: Refresh tokens are used to obtain new access and refresh token pairs when the current access token expires.
Token Lifetime: The default lifetime for refresh tokens is 24 hours for single-page applications and 90 days for other scenarios.





304
Conditional Access
Conditional Access: Client certificates can be used in conditional access policies to control access based on device compliance. This adds an extra layer of security by ensuring that only compliant devices can access sensitive data.





305
Conditional Access
Conditional Access App Control enables user app access and sessions to be monitored and controlled in real time based on access and session policies. Access and session policies are used within Microsoft Defender for Cloud Apps portal to further refine filters and set actions to be taken on a user.





306
Conditional Access
Report-only: Success	
All configured policy conditions, required non-interactive grant controls, and session controls were satisfied. For example, a multifactor authentication requirement is satisfied by an MFA claim already present in the token, or a compliant device policy is satisfied by performing a device check on a compliant device.





307
Dataverse Queries
FetchXML
Definition: A proprietary query language used in Dataverse for more advanced querying capabilities. Although the Web API primarily supports OData queries, FetchXML can be used for complex querying scenarios such as joins, aggregates, and advanced filtering.
Relevance: FetchXML is particularly useful for scenarios where OData query limitations are encountered. It can be executed using the Web API through the RetrieveMultiple message.





308
Dataverse Queries
Pagination and $top
Definition: Pagination refers to breaking down large sets of data into smaller, more manageable chunks. The $top query parameter controls how many records are returned in a single request.
Relevance: For performance reasons, Dataverse limits the number of records returned in a single API request. Pagination, along with $top, helps in retrieving large datasets incrementally without overwhelming the system.





309
Dataverse Queries
Entity Bound Actions
Definition: These are custom or system-defined actions that are tied to a specific entity and can be invoked via the Web API to perform tasks or trigger workflows.
Relevance: Entity-bound actions are useful for performing more complex business logic or automation that is triggered based on a record in an entity. For example, triggering a workflow on an engagement task when it's completed.





310
Dataverse Queries
_ts_engagementid_value@Microsoft.Dynamics.CRM.lookuplogicalname
Definition: A field that contains the logical name of the entity that the lookup field points to. This helps identify the type of entity related to the GUID in the _ts_engagementid_value field.
Example: The value "ts_engagement" indicates that the lookup points to the ts_engagement entity.





311
Dataverse Queries
Optimistic Concurrency
Definition: A method of ensuring that data integrity is maintained when multiple users or processes are updating the same record. It checks if the record has been modified since it was last retrieved before allowing updates.
Relevance: This is useful for preventing conflicts in multi-user environments. The Web API uses the If-Match header to implement this mechanism, ensuring that updates happen only if the record has not been altered by another process.





312
Dataverse Queries
Change Tracking
Definition: A feature that allows the tracking of changes made to data in Dataverse entities. The Web API exposes a mechanism to track when records have been created, updated, or deleted.
Relevance: Useful for scenarios like synchronizing data between systems or auditing changes in your data. The $deltatoken query parameter is used to fetch only the changes since the last request, improving efficiency.





313
Dataverse Queries
Lookup Field
Definition: A special type of field in Dataverse that references a record from another entity (table). It creates a relationship between two entities.
Example: ts_engagementid is a lookup field that points to a record in the ts_engagement entity.





314
Azure AI Search
AI-Powered Data Enrichment: "Skills" in Azure AI Search refer to AI functions that enhance and transform data during indexing, such as entity recognition, language detection, and sentiment analysis. These skills enrich the data to improve search relevance.





315
Azure AI Search
Comprehensive Data Enhancement: The enrichment pipeline can include various operations such as text extraction, translation, entity recognition, and sentiment analysis. These enhancements improve the quality and relevance of the indexed data, leading to better search outcomes.





316
Azure AI Search
Advanced Semantic Representations: Text-embedding-3 models convert textual data into dense vector representations, capturing the contextual and semantic nuances of the content. This enables Azure AI Search to perform more sophisticated semantic searches.





317
Azure AI Search
Structured Data Integration: "Mappings" define how source data fields are aligned with search index fields. Proper mappings ensure that data is correctly structured and accessible, facilitating efficient querying and accurate retrieval within Azure AI Search.





318
Azure AI Search
Efficient Text Processing: The Text Split skill breaks down large text documents into smaller, manageable token chunks. This facilitates more efficient indexing and processing within Azure AI Search, ensuring that each segment is handled optimally.





319
Azure AI Search
Detailed Diagnostic Insights: Debug Sessions offer access to logs and diagnostic information, enabling the detection of problems such as incorrect data mappings, skill misconfigurations, or performance bottlenecks, thereby improving the overall reliability of the search solution.





320
pandas
Error Handling and Performance with json_normalize: Using the errors parameter in json_normalize, you can manage parsing errors with options like 'raise' or 'ignore'. Efficient handling of large JSON datasets with json_normalize improves performance, making it valuable for big data applications.





321
pandas
Column-Specific Operation with explode: You can apply explode to a specific column by passing the column name to DataFrame.explode. Other columns remain unaffected, with their values duplicated alongside the new rows, maintaining the DataFrame's structural integrity.





322
pandas
Custom Separators and Maximum Depth in json_normalize: The sep parameter in json_normalize lets you define a custom separator for nested keys, defaulting to a dot (.). The max_level parameter allows you to limit how deeply json_normalize flattens nested data, providing control over the normalization process.





323
pandas
Handling Empty Lists and Missing Data in explode: The explode method gracefully handles empty lists and missing values (NaN) when you use DataFrame.explode. Empty lists result in no additional rows, while NaN values are propagated, ensuring DataFrame consistency after the operation.





324
pandas
Exploding Multiple Columns using explode: Starting from pandas version 1.3.0, DataFrame.explode allows you to explode multiple columns simultaneously by passing a list of column names to explode. This is useful when you have multiple list-like columns that need to be expanded in parallel.





325
pandas
There is an attribute on a dataframe column to check if each row has a unique value:

df['colname'].isunique





326
pandas
# Drop columns with only one unique value
df = df.loc[:, df.nunique() > 1]





327
pandas
DataFrame.nunique(axis=0, dropna=True)
    Count number of distinct elements in specified axis.
    Return Series with number of distinct elements. Can ignore NaN values.

axis: {0 or ‘index’, 1 or ‘columns’}, default 0
dropna: bool, default True, Don’t include NaN in the counts.





328
AKS
AKS is commonly used to host multiple workloads or disparate workload components. You can isolate these workloads and components by using Kubernetes native functionality, like namespaces, access controls, and network controls, to meet security requirements.





329
AKS
Although AKS offers the most customization, it demands greater operational input. In contrast, PaaS solutions like Container Apps and Web App for Containers let Azure handle tasks like OS updates.





330
AKS
Web App for Containers is a feature of Azure App Service, a fully managed service for hosting HTTP-based web apps with built-in infrastructure maintenance, security patching, scaling, and diagnostic tooling. For more information, see App Service documentation.





331
AKS
Azure Kubernetes Service (AKS) is a managed Kubernetes service for running containerized applications. With AKS, you can take advantage of managed add-ons and extensions for additional capabilities while preserving the broadest level of configurability. For more information, see AKS documentation.





332
AKS
Azure Container Apps is a fully managed Kubernetes-based application platform that helps you deploy HTTP and non-HTTP apps from code or containers without orchestrating infrastructure. For more information, see Azure Container Apps documentation.





333
AKS
Application scalability in AKS is the sole responsibility of the customer. Container Apps and Web App for Containers offer more streamlined approaches.





334
AKS
AKS also provides access to the Kubernetes API server, which enables you to customize the container orchestration and thus deploy projects from the Cloud Native Computing Foundation (CNCF). Consequently, there's a significant learning curve for workload teams that are new to Kubernetes.





335
AKS
Web App for Containers and Container Apps health probe configurations are more streamlined than those of AKS, given that they use the familiar Azure Resource Manager API. AKS requires the use of the Kubernetes API.





336
Azure OpenAI
Assistants can access files in several formats. Either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (such as images or spreadsheets) and cite files they reference in the Messages they create.





337
Azure OpenAI
Assistants API supports persistent automatically managed threads. This means that as a developer you no longer need to develop conversation state management systems and work around a model’s context window constraints. The Assistants API will automatically handle the optimizations to keep the thread below the max context window of your chosen model.





338
Azure OpenAI
Assistant 
Custom AI that uses Azure OpenAI models in conjunction with tools.

Message
A message created by an Assistant or user. Messages can include text, images, and other files. Messages are stored as a list on the Thread.





339
Azure OpenAI
Run
Activation of an Assistant to begin running based on the contents of the Thread.

Run Step
A detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create messages during its run. Examining Run Steps allows you to understand how the Assistant is getting to its final results.





340
Azure OpenAI
Thread
A conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model's context.





341
Azure OpenAI
Assistants can call Azure OpenAI’s models with specific instructions to tune their personality and capabilities.
Assistants can access multiple tools in parallel. These can be both Azure OpenAI-hosted tools like code interpreter and file search, or tools you build, host, and access through function calling.





342
Dataverse Plugins
The Plugin Execution Pipeline defines the sequence of events where plugins can be triggered in Dataverse. The primary stages include:

Pre-validation: Executes before the main system operation, allowing for early validation.
Pre-operation: Runs after the main operation is validated but before it's committed to the database.
Post-operation: Occurs after the main operation has been completed and committed. Understanding these stages is crucial for placing plugins at the appropriate point in the data processing workflow, ensuring optimal performance and data integrity.





343
Dataverse Plugins
Filtering Attributes are specific fields defined during the registration of a plugin that determine when the plugin should execute. For instance, a plugin might be set to trigger only when certain attributes of an entity are updated. By specifying these attributes, Dataverse optimizes performance by invoking the plugin only when relevant data changes occur, rather than on every update to the entity.





344
Dataverse Plugins
Plugins are custom business logic components that execute in response to specific events within Dataverse, such as record creation or updates. Written in .NET, plugins can enforce business rules, validate data, or integrate with external systems. They are registered against specific messages (like Create, Update) and entities, allowing for granular control over when and how custom logic is triggered within the Dataverse environment.





345
Dataverse Plugins
SDK Message Processing Steps represent the registration details of a plugin within Dataverse. Each step defines when (which message and stage) and how a plugin executes. Key properties include the message (e.g., Update), primary entity, execution pipeline stage (Pre-validation, Pre-operation, Post-operation), and filtering attributes. These steps orchestrate the plugin's behavior in response to data operations.





346
Dataverse Plugins
API Permissions and Scopes define what operations an application can perform and what data it can access within an API like Dataverse Web API. During Azure AD application registration, permissions are granted (e.g., user_impersonation) to control access levels. Scopes specify the exact permissions requested during the authentication process. Proper configuration ensures that applications have the necessary access without over-privileging, adhering to the principle of least privilege for security.





347
Dataverse Plugins
Error Handling in Dataverse involves interpreting and managing error messages returned by the system during operations. Errors can arise from validation failures, plugin restrictions, permission issues, or data conflicts. Effective error handling requires understanding error codes, messages, and contextual details provided by the system. This enables developers to debug issues, enforce business rules, and provide meaningful feedback to users or calling applications.





348
Dataverse Plugins
The PluginType entity in Dataverse stores metadata about registered plugins. It includes information such as the plugin's unique identifier (plugintypeid), name, assembly details, and the class implementing the plugin logic. By querying the plugintypes entity via the Web API, developers can retrieve information about specific plugins, facilitating management and debugging of custom business logic.





349
Diffusers Documentation
On Windows, pip relies on a small launcher executable that links to the correct Python interpreter and pip package. If Python was uninstalled and reinstalled in a new location, the link in that launcher may be broken or still pointing to the old installation path, even if the correct paths are in the Path variable. This causes pip alone to fail, as it tries to find the old, non-existent Python.





350
Diffusers Documentation
Schedulers manage going from a noisy sample to a less noisy sample given the model output - in this case, it is the noisy_residual.





351
Diffusers Documentation
The Stable Diffusion model is a good starting point, and since its official launch, several improved versions have also been released. However, using a newer version doesn’t automatically mean you’ll get better results. You’ll still have to experiment with different checkpoints yourself, and do a little research (such as using negative prompts) to get the best results.





352
Diffusers Documentation
Choosing a more efficient scheduler could help decrease the number of steps without sacrificing output quality. You can find which schedulers are compatible with a different model in the DiffusionPipeline by calling the compatibles method:

pipeline.scheduler.compatibles





353
Diffusers Documentation
The Scripts\ folder contains the pip executable. If there’s any issue in the way Path variables are processed (which can occasionally happen if multiple Python versions are installed), running python -m pip ensures the right pip is used with the right Python version.





354
Dataverse
Event-Driven Architecture: Plug-ins operate on an event-driven architecture, meaning they are triggered by specific events within the Dataverse environment. These events can be actions like data creation, update, or deletion. Understanding the event model is crucial for effectively implementing and managing plug-ins.





355
Dataverse
Authorization Code Flow:

Definition: An OAuth2 flow used by public clients (like single-page apps or mobile apps) where the user signs in interactively, providing credentials, and possibly undergoing multi-factor authentication (MFA).
Context: We discussed this flow as an alternative to the Client Credentials Flow because you wanted MFA to be required.





356
Dataverse
Bearer Token:

Definition: A type of token used in OAuth2 that authorizes access to resources like APIs.
Context: The token obtained from Azure AD that you include in the Authorization header for your API requests.





357
Dataverse
Public Client:

Definition: An application type in OAuth2 that does not require a client secret because it runs on devices that cannot securely store credentials (like mobile or desktop apps).
Context: We configured your app as a public client so it could authenticate without a client secret





358
Dataverse
MSAL (Microsoft Authentication Library):

Definition: A Microsoft library for handling authentication and obtaining tokens using OAuth2. It simplifies the process of integrating OAuth2 with applications.
Context: We used MSAL in Python to implement the Device Code Flow and authenticate interactively with MFA.





359
Git
git status: This command shows the current state of your working directory and staging area. It helps you see which changes have been staged, which haven't, and which files aren't being tracked by Git.





360
Git
git pull: This command fetches and integrates changes from a remote repository into your local repository. It's a combination of git fetch and git merge.





361
Git
git init: This command initializes a new Git repository in your current directory. It creates a hidden .git directory that tracks all changes to your project.





362
Git
git diff: This command shows the differences between commits, branches, or your working directory and the staging area. It's useful for reviewing changes before committing them.





363
Git
git stash: This command temporarily saves changes in your working directory that you aren't ready to commit. You can later apply or discard these changes.





364
Workera.ai
Azure IoT Hub
Purpose and Use Cases:

Core IoT Service: Azure IoT Hub is a foundational service designed to provide the connectivity and messaging infrastructure required for building sophisticated IoT solutions.
Customization: It's ideal for developers who need a highly customizable solution and have the expertise to build and manage the IoT application layer themselves.





365
Workera.ai
Azure HDInsight: Suitable for diverse big data processing needs with open-source frameworks but requires infrastructure management.





366
Workera.ai
Azure DevTest Labs enables quick provisioning of resources such as virtual machines, allowing development and testing teams to create environments on-demand. This is particularly useful for scenarios where teams need to replicate production environments for testing purposes.





367
Workera.ai
Automatic Replication and Failover: Certain services, such as Azure Storage, automatically replicate data between region pairs. This replication helps ensure that your data is always available, even if one region experiences an outage. Additionally, Azure prioritizes the restoration of services in region pairs to ensure a quicker recovery time.





368
Workera.ai
Geographical Separation: Azure Region Pairs consist of two regions within the same geography but separated by a significant distance to protect against regional disasters. This ensures that in the event of a natural disaster or significant outage in one region, the paired region remains operational, thereby providing enhanced disaster recovery capabilities.





369
Jane Eyre
Propitious (pruh-PISH-uhs)
Favorable or advantageous, indicating circumstances or timing that are likely to lead to success or good outcomes. Often used to describe auspicious events or benevolent actions.





370
Jane Eyre
Dun and Sere
"Dun" refers to a dull grayish-brown color, often describing animals or landscapes. "Sere" means dry or withered, typically describing vegetation. Together, the phrase evokes an autumnal or barren scene.





371
Jane Eyre
Cachinnation (kak-ih-NAY-shun)
Loud, uncontrollable laughter, often with a connotation of being excessive, mocking, or even sinister. Derived from the Latin word cachinnare, it emphasizes the intensity of the laugh.





372
Jane Eyre
Gytrash (JAI-trash)
A mythical creature from English folklore, typically appearing as a ghostly, shape-shifting animal such as a horse, dog, or mule. Known for haunting travelers on lonely roads, it often carries a mysterious or ominous aura.





373
Jane Eyre
Hips and Haws
Fruits of wild roses (hips) and hawthorn trees (haws), commonly seen in hedgerows. These red or orange berries are associated with autumn and are used in herbal remedies and preserves.





374
Jane Eyre
Prenomen (pree-NO-men)
A person's first or given name, distinguishing them within a family or community. Derived from Latin, the term historically referred to the personal name in ancient Roman naming conventions.





375
Jane Eyre
Physiognomy (fiz-ee-OG-nuh-mee)
The study or art of interpreting a person's character or personality based on their facial features or expressions. Historically linked to pseudoscience, it was once used to infer traits or predict behaviors.





376
Jane Eyre
Canzonette (kan-zoh-NET)
A short, light, and often playful Italian song, typically sung in a simple style. Originating during the Renaissance, canzonettes were precursors to madrigals, characterized by their charming melodies and accessible lyrics.





377
Jane Eyre
Merino (muh-REE-noh)
A breed of sheep prized for its soft, fine wool, ideal for high-quality textiles. The term also refers to fabrics or garments made from this wool, valued for their warmth and luxurious texture.





378
Jane Eyre
Efface
To erase or remove something, especially from a surface or memory, making it less visible or prominent. Often used figuratively to describe fading memories or diminishing the impact of an event or object.





379
General
Sectoral Diversity: Within the European Stoxx 600, there is a rich mix of sectors represented, including financial services, healthcare, industrials, technology, and consumer goods. This diversity not only mitigates investment risk but also offers investors comprehensive exposure to the varied economic sectors across Europe.





380
General
Automatic VM guest patching: This refers to the automated process of updating the operating system of virtual machines hosted on Azure to ensure security and compliance with the latest patches.





381
General
[Win] + [ctrl] + [v] opens the windows audio device selection menu in Windows 11. Very helpful.





382
General
Sideloading benefits include testing new app features, deploying custom enterprise apps, and accessing legacy applications critical to business operations, despite not being available in app stores.





383
Developer Mode
The Windows App SDK was previously known by the code name Project Reunion. Some SDK assets (such as the VSIX extension and certain NuGet packages) still use this name, but these assets will be renamed in a future release. Some documentation still uses Project Reunion when referring to an existing asset or a specified earlier release.





384
Developer Mode
When you enable Developer Mode on desktop, a package of features is installed, including:

Windows Device Portal: Device Portal is only enabled (and firewall rules are only configured for it) when the Enable Device Portal option is turned on.
Installs and configures firewall rules for SSH services that allow remote installation of apps. Enabling Device Discovery will turn on the SSH server.





385
Email Auth and Security
p=none

A DMARC policy action.
Requests no specific action be taken on unauthenticated emails.
Used for monitoring and gathering data without affecting email delivery.





386
Email Auth and Security
MUA (Mail User Agent)

An email client used by end-users to send, receive, and manage their emails.
Provides the interface for users to interact with their email accounts.
Examples include Microsoft Outlook, Mozilla Thunderbird, and Gmail.





387
Email Auth and Security
Identifier Alignment

Ensuring that the domains used in SPF and DKIM authentication align with the domain in the email's "From" header.
Critical for DMARC to function correctly.
Can be achieved through relaxed alignment (subdomains are allowed) or strict alignment (exact match required).





388
Email Auth and Security
Forensic Reports (Failure Reports)

Detailed reports sent to domain owners for individual emails that fail DMARC authentication.
Offer granular information to help diagnose and address specific authentication failures.
May include original email headers and details about the failure reasons.





389
Email Auth and Security
DKIM (DomainKeys Identified Mail)

An email authentication method using cryptographic signatures.
Adds a digital signature to email headers to verify the email's authenticity and integrity.
Ensures that the email content has not been altered during transit.





390
Email Auth and Security
Feedback Loops (FBL)

Mechanisms by which email service providers notify senders about spam complaints from recipients.
Allow senders to monitor and address spam-related issues, improving sender reputation and email deliverability.
Implemented through services like Return Path and Feedback Loop (FBL) agreements with ISPs.





391
Email Auth and Security
SMTP (Simple Mail Transfer Protocol)

The standard protocol used for sending emails across the internet.
Facilitates the transmission of email messages from senders to recipients via mail servers.
Includes commands like HELO/EHLO, MAIL FROM, RCPT TO, DATA, and QUIT.





392
Email Auth and Security
HELO/EHLO

SMTP commands used by a mail server to identify itself to another mail server.
Initiates the SMTP conversation and begins the process of sending an email.
EHLO is an extended version that enables additional SMTP features.





393
Email Auth and Security
Email Spoofing

The creation of email messages with a forged sender address.
Deceives recipients into believing the email is from a legitimate source.
Commonly used in phishing attacks to trick users.





394
Email Auth and Security
Aggregate Reports

Summary reports sent periodically (usually daily) to domain owners detailing email authentication results.
Provide insights into SPF and DKIM alignment and the overall effectiveness of DMARC policies.
Typically formatted in XML, containing data on email sources, authentication outcomes, and policy actions.





395
Email Auth and Security
p=quarantine

A DMARC policy action.
Instructs receiving mail servers to treat unauthenticated emails with suspicion.
Typically places such emails in the spam or junk folder.





396
Email Auth and Security
Email Authentication Alignment

Ensuring that SPF and DKIM align with the domain specified in the email's "From" header.
Critical for DMARC to function correctly.
Provides consistent authentication results and improves email deliverability.





397
Email Auth and Security
SPF (Sender Policy Framework)

A DNS-based email authentication method.
Specifies which mail servers are authorized to send emails on behalf of a domain.
Helps prevent email spoofing by verifying the sender's IP address against the domain's SPF record.





398
Email Auth and Security
DMARC (Domain-based Message Authentication, Reporting & Conformance)

Builds on SPF and DKIM to provide domain owners with policies for handling unauthenticated emails.
Enables reporting on email authentication status and issues.
Protects against unauthorized use of a domain in email communications.





399
Email Auth and Security
BIMI (Brand Indicators for Message Identification)

Allows organizations to display their brand logos alongside authenticated emails in recipients' inboxes.
Enhances brand recognition and trust.
Requires proper implementation of SPF, DKIM, and DMARC.





400
Email Auth and Security
DMARC Analyzer

A tool that helps organizations monitor and analyze DMARC reports.
Provides insights into email authentication performance.
Assists in enforcing DMARC policies effectively.





401
Email Auth and Security
Forwarding Services

Third-party services that forward emails from one address to another.
Can interfere with SPF checks since the forwarding server's IP may not be authorized in the original domain's SPF record.
DKIM signatures can survive forwarding if the email content remains unchanged, mitigating some issues.





402
Email Auth and Security
p=reject

A DMARC policy action.
Instructs receiving mail servers to reject unauthenticated emails outright.
Provides the strictest protection against fraudulent emails.





403
Email Auth and Security
ARC (Authenticated Received Chain)

Preserves email authentication results as messages pass through multiple intermediary services.
Helps maintain the integrity of authentication information when emails are forwarded or processed by third parties.
Prevents legitimate emails from being marked as unauthenticated.





404
Email Auth and Security
Mail Delivery Agent (MDA)

Software responsible for delivering emails to the recipient's mailbox.
Receives emails from the Mail Transfer Agent (MTA) and places them in the appropriate mailbox or folder.
Examples include Procmail, Dovecot, and Maildir.





405
Email Auth and Security
Email Deliverability

The ability of an email to successfully reach the recipient's inbox.
Influenced by factors like email authentication, sender reputation, and content quality.
Ensures that legitimate emails are accessible to recipients without being marked as spam.





406
Email Auth and Security
PGP (Pretty Good Privacy)

A data encryption and decryption program that provides cryptographic privacy and authentication.
Used for securing emails through encryption and digital signatures.
Combines symmetric and asymmetric encryption for robust security.





407
Email Auth and Security
TXT Record

A type of DNS record used to store text information.
Publishes SPF policies, DKIM public keys, and DMARC policies.
Essential for configuring email authentication protocols.





408
Email Auth and Security
MX Toolbox

An online suite of tools for diagnosing and analyzing DNS records.
Checks SPF, DKIM, and DMARC configurations.
Helps administrators verify the correctness of their email authentication setups.





409
Email Auth and Security
Selector (DKIM)

A label used in DKIM to specify which DKIM key to use for verifying an email.
Allows multiple DKIM keys to exist for a single domain, facilitating key rotation and management.
Included in the DKIM-Signature header as the "s=" tag.





410
IW Migration
Stored Procedures and Triggers
Database components that contain reusable SQL code and logic. Stored procedures perform complex queries or transformations. Triggers automatically run when certain table events occur (like insert, update, delete), enforcing rules or updating related data behind the scenes.





411
IW Migration
SSIS (SQL Server Integration Services)
A platform for building enterprise-level data integration, transformation, and migration workflows. While not directly in SSMS, SSIS packages can orchestrate complex ETL operations, complementing Access-to-SQL-Server migrations and ongoing data maintenance.





412
IW Migration
Check Constraints and Computed Columns (SQL Server)
Check constraints enforce custom rules on column values, ensuring data integrity at the database level. Computed columns derive their values from an expression referencing other columns, reducing redundant storage and improving data consistency.





413
IW Migration
Indexing (Access/SQL Server)
A strategy for improving data retrieval speeds by creating data structures (indexes) on certain fields. Proper indexing balances query performance gains against the overhead of maintaining those indexes during inserts, updates, and deletions.





414
IW Migration
SSMS Object Explorer
A navigation pane in SQL Server Management Studio that organizes database objects—tables, views, stored procedures, and security settings—into a hierarchical structure. It provides quick access to browse, modify, and manage all database components.





415
IW Migration
Dynamic SQL (SQL Server)
SQL commands constructed and executed at runtime. Used for flexible, parameter-driven logic, it’s powerful but requires careful handling to prevent SQL injection vulnerabilities and to maintain clarity and performance in complex code.





416
IW Migration
Maintenance Plans (SSMS)
Configurable workflows in SQL Server Management Studio that automate routine database tasks—like index reorganizations, integrity checks, and backups. They help maintain database health and performance without requiring constant manual intervention from administrators.





417
IW Migration
Functions (User-Defined Functions) (SQL Server)
Reusable routines returning a scalar value or a table. They allow encapsulation of logic into named units, improving code maintainability. However, careful use is required as certain functions can impact query performance if not optimized.





418
IW Migration
SSMS Query Editor
A text-based interface in SQL Server Management Studio where you can write, run, and optimize SQL queries. It supports syntax highlighting, IntelliSense, and execution plans, helping users test logic, retrieve data, and troubleshoot issues efficiently.





419
IW Migration
VBA Modules (Access)
Visual Basic for Applications code libraries integrated within Access. Modules let developers write custom functions, event handlers, and business logic to enhance functionality beyond what macros offer, enabling more advanced data manipulation and validation.





420
IW Migration
Template Explorer (SSMS)
A feature in SQL Server Management Studio providing ready-made SQL code templates for common database tasks. Developers can quickly insert standardized scripts for creating tables, views, or procedures, streamlining development and enforcing coding standards.





421
IW Migration
Property Sheets (Access)
Panels that display and allow editing of properties for forms, reports, and controls. Users can customize appearance, behavior, event triggers, and data sources to tailor the interface and user experience to specific business requirements.





422
IW Migration
Linked Table Manager (in Access)
A feature in MS Access that helps you manage and refresh connections to external data sources. If a database is split, this tool reconnects front-end objects to back-end tables, ensuring smooth data retrieval and updates.





423
IW Migration
Security (SSMS)
Tools and configurations for managing user access, roles, and permissions within SQL Server Management Studio. Administrators can control who can view, modify, or delete data, maintain compliance with policies, and ensure the database’s overall integrity.





424
IW Migration
Forms (Access)
User-friendly interfaces for entering, displaying, and editing data in tables. They can incorporate buttons, dropdowns, and validation rules, making data input more efficient and consistent, and often serve as the primary user interaction layer.





425
IW Migration
Transaction Handling (SQL Server)
A mechanism to ensure that a series of database operations either all succeed or all fail, maintaining data integrity. Using commands like BEGIN TRAN, COMMIT, and ROLLBACK, developers control atomic operations and safeguard against partial updates.





426
IW Migration
Execution Plans (SQL Server)
Visual or textual representations of how the SQL Engine executes queries. They identify steps taken—like table scans, index seeks, or joins—helping developers and DBAs optimize performance, tune queries, and refine indexing strategies.





427
IW Migration
Database Diagrams (SSMS)
Graphical representations of a database’s schema. They visually display tables, columns, keys, and relationships, aiding comprehension of complex data models, enhancing communication among developers, and assisting in planning changes or migrations to other systems.





428
IW Migration
Materialized Views (Indexed Views) (SQL Server)
Views physically stored as indexed structures for faster retrieval of complex queries. Unlike normal views, indexed views can greatly improve performance for frequently accessed aggregates or joins, though they require careful maintenance and design.





429
IW Migration
Pass-Through Queries (Access)
A type of query that sends SQL commands directly to an external database (e.g., SQL Server). Bypassing Access’s query engine, they offer better performance and exploit native database features, ideal when dealing with large datasets or complex logic.





430
IW Migration
Linked Table Manager (Access)
A utility in Access to manage external data connections. It allows refreshing, updating, or changing the source of linked tables, ensuring that front-end objects always correctly reference tables in the back-end or external databases.





431
IW Migration
Database Backup and Restore (SSMS)
Built-in wizards and features in SQL Server Management Studio to safeguard data. They allow administrators to create full, differential, or transaction log backups and restore databases to a previous state, ensuring data recovery after failures.





432
IW Migration
Linked Server (SQL Server)
A configuration that allows SQL Server to run queries against remote data sources as if they were local tables. This enables federated queries, joining data across different servers and even different database platforms.





433
IW Migration
Refactoring and Normalization (Access/SQL Server)
The process of reorganizing tables and fields to reduce redundancy and improve data integrity. Higher normalization forms minimize duplicate data and anomalies. This often improves long-term maintainability and eases migration to other systems.





434
IW Migration
Syntax to find files that contain a substring. Needed this when trying to remember where I made certain files when working on IW files.
Get-ChildItem -Recurse | Select-String "account_edit" -List | Select Path





435
IW Migration
Table Design View (Access)
A layout within Access for defining fields, data types, validation rules, and default values for a table’s structure. It lets users carefully shape the database schema to enforce data integrity and consistency from the start.





436
IW Migration
Locking and Concurrency Control (SQL Server)
Techniques to manage simultaneous data access, preventing conflicts and ensuring consistency. Locks, isolation levels, and optimistic concurrency are tools that balance performance with data correctness when multiple users edit data simultaneously.





437
IW Migration
Macros (Access)
Automated actions or sequences of commands within Access. They enable non-programmers to implement logic—like opening forms, running queries, or updating records—without writing code, providing a streamlined way to standardize tasks and workflows.





438
Azure AI Foundry
Azure OpenAI: Provides access to the latest Open AI models. You can create secure deployments, try playgrounds, fine tune models, content filters, and batch jobs. The Azure OpenAI resource provider is Microsoft.CognitiveServices/account and the kind of resource is OpenAI.





439
Azure AI Foundry
Azure AI Studio is no Azure AI Foundry portal. We're updating the documentation to reflect this change. In the meantime, you might see references to Azure AI Studio.





440
Azure AI Foundry
When using Azure AI Foundry portal, you can directly work with Azure OpenAI without an Azure Studio project or you can use Azure OpenAI through a project.





441
Azure AI Foundry
AI Foundry project: A project is a child resource of the hub. The Azure resource provider for a project is Microsoft.MachineLearningServices/workspaces, and the kind of resource is Project. The project provides the following features:
    Access to development tools for building and customizing AI applications.
    Reusable components including datasets, models, and indexes.
    An isolated container to upload data to (within the storage inherited from the hub).





442
Azure AI Foundry
AI Foundry hub: The hub is the top-level resource in AI Foundry portal and is based on the Azure Machine Learning service. The Azure resource provider for a hub is Microsoft.MachineLearningServices/workspaces, and the kind of resource is Hub.