https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
Enable yoru device for development

Important 
If you're not creating your own applications on your PC, you don't need to enable Developer Mode. If you're trying to fix an issue with your computer, check out Windows help.

If you're using your computer for ordinary day-to-day activities (such as gaming, web browsing, email, or Office apps), you don't need to activate Developer Mode, and in fact, you shouldn't activate it. The rest of the information on this page won't matter to you, and you can safely get abck to whatever you were doing. Thanks for stopping by!

Developer Mode

If you're writing software in Visual Studio on a computer for the first time, you WILL need to envable Developer Mode on both the development PC and on any devices you'll use to test your code. Opening a Windows project when Developer Mode isn't enabled will either oepn the For developers setting page, or cause the following dialog box to appear in Visual Studio:

If you see this dialog, select settings for developers to open the For developers settings page.

Note
You can go to the For developers settings page at any time to enable or disable Developer Mode. Simply enter for developers into the search box in the task bar.

Activate Developer Mode

To enable Developer Mode, or access other settings:

1. Toggle the Developer Mode setting, at the top of the For developers page.
2. Read the disclaimer for the setting you choose. Click Yes to accept the change.

Developer Mode features

Developer Mode replaces the Windows 8.1 requirements for a developer licesne. In addition to sideloading, the Developer Mode setting enables debugging and additional deployment options. This includes starting an SSH service to allow deployment to this device. In order to stop this service, you need to disable Developer Mode.

...

https://learn.microsoft.com/en-us/windows/apps/windows-app-sdk/system-requirements#:~:text=System%20requirements%20for%20Windows%20app%20development

To develop apps for Windows 10 and 11, you'll need Visual Studio, the Windows SDK, and Windows App SDK. Before installing these tools, make sure your development computer meets hte minimum system requirements.

Install the tools for the Windows App SDK to get started.

Visual Studio

Visual Stuido is a comprehensive Integrated Development Environment (IDE) that's used to edit, debug, build, and publish apps.

For the minimum system requirements, see:

    Visual Studio 2022 system requirements
    Visual Studio 2019 system requirements

Windows SDK

The Windows SDK provides access to all of the APIs and development featuers exposed by the Windows OS. The Windows SDK is required for building Windows apps as well as other types of components (such as services and drivers). The latest Windows SDK is installed with Visual Studio 2019 and Visual Studio 2022 by default.

For the minimum system requirements, see Windows SDK.

Windows App SDK 

The Windows App SDK is a set of developer tools that represent the next evolution in hte Windows app development platform. It provides a unified set of APIs and tools that can be used in a consistent way by any desktop app on Windows 11 (and it's backward compatible for Windows 10, version 1809).

Note
The Windows App SDK

...

In the example above, the pipeline contains a UNet2DModel and a DDPMScheduler. The pipeline denoises an image by taking random noise the size of the desired output and passing it through the model several times. At each timestep, the model predicts the noise residual and the scheduler uses it to predict a less noisy image. The pipeline repeats this process until it reaches the end of the specified number of inference steps.

To recreate the pipeline with the model and scheduler separately, let's write our own denoising process. 

1. Load the model and scheduler

from diffusers import DDPMScheduler, UNet2DModel
scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256", use_safetensors=True).to("cuda")

2. Set the number of timesteps to run the denoising process for:

scheduler.set_timesteps(50)

3. Setting the scheduler timesteps creates a tensor with evenly spaced elements in it, 50 in this example. Each element corresponds to a timestep at which the model denoises an image. When you create the denoising loop later, you'll iterate over this tensor to denoise an image:

scheduler.timesteps

4. Create some random noise with the same shape as the desired output:

import torch
sample_size = model.config.sample_size
noise = torch.randn((1,3, sample_size, sample_size), device="cuda")

5. Now wrtie a loop to iterate over the timesteps. At each timestep, the model does a UNet2DModel.forward() pass and returns the noisy residual. The scheduler's step() method takes the noisy residual, timestep, and input and it predicts the image at the previous timestep. This output becomes the next input to the model in the denoising loop, and it'll repeat until it reaches the end of the timesteps array.

input = noise
for t in scheduler.timesteps:
    with torch.no_grad():
        noisy_residual = model(input, t).sample
    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
    input=previous_noisy_sample 

This is the entire denoising process, and you can use this same pattern to write any diffusion system. 

6. The last step is to convert the denoised output to an image:

from PIL import image
import numpy as np

image = (input / 2 + 0.5).clamp(0,1).squeeze()
image = (image.permut(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()
image = Image.fromarray(image)
image

...

Create random noise 

Next, generate some initial random moise as a starting point for the diffusion process. This is the latent representation of the image, and it'll be gradually denoised. At this point, the latent image is smaller than teh final image size, but that's ok because the model will transform it into the final 512x512 image dimensions later.

The height and width are divided by 8 because the vae model has 3 down-sampling layers. You can check by running the following:

2 ** (len(vae.config.block_out_channels) -1_ == 8)

latents = torch.randn(
    (batch_size, unet.config.in_channels, height //8, width//8),
    generator=generator,
    device=torch_device
)

Denoise the image 

Start by scaling the input with the initial noise distribution, sigma, the noise scale value, which is required for improved scheduelrs like uniPCMultistepScheduler:

latents = latents * scheduler.init_noise_sigma

The last step is to create the denoising loop that'll progressively transform the pure noise in latents into an image described by your prompt. Remember, the denoising loop needs three things:

1. Set the scheduler's timesteps to use during denoising.
2. Iterate over the timesteps.
3. At each timestep, call the UNet model to predict the noise residual and pass it to the scheduler to compute the previous noisy sample.

from tqdm.auto import tqdm 

scheduler.set_timesteps(num_inference_steps)

```
for t in tqdm(scheduler.timesteps):
    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes
    latent_model_input = torch.cat([latents] * 2)
    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)

    # predict the noise residual.
    with torch.no_grad():
        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

    # perform guidance 
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

    # compute the previous noisy sample x_t -> x_t-1
    latents = scheduler.step(noise_pred, t, latents).prev_sample 

```

Decode the image 

The final step is to use the vae to decode the latent representation into an image and get the decoded output with sample:

```
# scale and decode the image latents with vae 
latents = 1/ 0.18215 * latents 
with torch.no_grad()
    image = vae.decode(latents).sample
```

Lastly, convert the iamge to a PIL.Image to see your generated image!

```
image = (image/2 + 0.5).calmp(0, 1).squeeze() 
image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()
image = Image.fromarray(image) 
image 
```

